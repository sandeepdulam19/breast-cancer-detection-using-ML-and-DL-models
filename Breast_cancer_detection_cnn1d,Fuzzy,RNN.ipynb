{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Breast_cancer_detection_cnn1d,Fuzzy,RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "VKOxZeo2vpM1"
      },
      "source": [
        "# Find the direction of dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzHWc9KyvpND",
        "outputId": "43738127-8382-4401-a34e-e72434ca7b71"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Conv1D, MaxPool1D, Flatten\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import AveragePooling1D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import random # for visualization\n",
        "print('Libraries Imported')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Libraries Imported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLC6w_13wra4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79d7876-ad68-4754-919e-a89a30ba4486"
      },
      "source": [
        "#import pictures from drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN_-lejyvpNG"
      },
      "source": [
        "path = '/content/drive/My Drive/Data/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xnwocimvpNH",
        "outputId": "97c50bfb-1f83-4ecd-9b14-e27ab8a23d83"
      },
      "source": [
        "print(\"reading dataframe\")\n",
        "info=pd.read_csv(\"/content/drive/My Drive/Data/data.csv\")\n",
        "info=info.drop('Unnamed: 32',axis=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading dataframe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "I9QH5n4jvpNI",
        "outputId": "59f1840f-4d12-4021-b0a8-ac14b8380449"
      },
      "source": [
        "info"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>926424</td>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>926682</td>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>926954</td>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>927241</td>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>92751</td>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows Ã— 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id diagnosis  ...  symmetry_worst  fractal_dimension_worst\n",
              "0      842302         M  ...          0.4601                  0.11890\n",
              "1      842517         M  ...          0.2750                  0.08902\n",
              "2    84300903         M  ...          0.3613                  0.08758\n",
              "3    84348301         M  ...          0.6638                  0.17300\n",
              "4    84358402         M  ...          0.2364                  0.07678\n",
              "..        ...       ...  ...             ...                      ...\n",
              "564    926424         M  ...          0.2060                  0.07115\n",
              "565    926682         M  ...          0.2572                  0.06637\n",
              "566    926954         M  ...          0.2218                  0.07820\n",
              "567    927241         M  ...          0.4087                  0.12400\n",
              "568     92751         B  ...          0.2871                  0.07039\n",
              "\n",
              "[569 rows x 32 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDcajV-yrmYo"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "info['diagnosis']=le.fit_transform(info['diagnosis'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9uBZS19Oremi",
        "outputId": "f8d45de6-1dda-44e6-ecbf-4c5fec9bae60"
      },
      "source": [
        "info.corr()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.039769</td>\n",
              "      <td>0.074626</td>\n",
              "      <td>0.099770</td>\n",
              "      <td>0.073159</td>\n",
              "      <td>0.096893</td>\n",
              "      <td>-0.012968</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.050080</td>\n",
              "      <td>0.044158</td>\n",
              "      <td>-0.022114</td>\n",
              "      <td>-0.052511</td>\n",
              "      <td>0.143048</td>\n",
              "      <td>-0.007526</td>\n",
              "      <td>0.137331</td>\n",
              "      <td>0.177742</td>\n",
              "      <td>0.096781</td>\n",
              "      <td>0.033961</td>\n",
              "      <td>0.055239</td>\n",
              "      <td>0.078768</td>\n",
              "      <td>-0.017306</td>\n",
              "      <td>0.025725</td>\n",
              "      <td>0.082405</td>\n",
              "      <td>0.064720</td>\n",
              "      <td>0.079986</td>\n",
              "      <td>0.107187</td>\n",
              "      <td>0.010338</td>\n",
              "      <td>-0.002968</td>\n",
              "      <td>0.023203</td>\n",
              "      <td>0.035174</td>\n",
              "      <td>-0.044224</td>\n",
              "      <td>-0.029866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>diagnosis</th>\n",
              "      <td>0.039769</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.730029</td>\n",
              "      <td>0.415185</td>\n",
              "      <td>0.742636</td>\n",
              "      <td>0.708984</td>\n",
              "      <td>0.358560</td>\n",
              "      <td>0.596534</td>\n",
              "      <td>0.696360</td>\n",
              "      <td>0.776614</td>\n",
              "      <td>0.330499</td>\n",
              "      <td>-0.012838</td>\n",
              "      <td>0.567134</td>\n",
              "      <td>-0.008303</td>\n",
              "      <td>0.556141</td>\n",
              "      <td>0.548236</td>\n",
              "      <td>-0.067016</td>\n",
              "      <td>0.292999</td>\n",
              "      <td>0.253730</td>\n",
              "      <td>0.408042</td>\n",
              "      <td>-0.006522</td>\n",
              "      <td>0.077972</td>\n",
              "      <td>0.776454</td>\n",
              "      <td>0.456903</td>\n",
              "      <td>0.782914</td>\n",
              "      <td>0.733825</td>\n",
              "      <td>0.421465</td>\n",
              "      <td>0.590998</td>\n",
              "      <td>0.659610</td>\n",
              "      <td>0.793566</td>\n",
              "      <td>0.416294</td>\n",
              "      <td>0.323872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_mean</th>\n",
              "      <td>0.074626</td>\n",
              "      <td>0.730029</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.323782</td>\n",
              "      <td>0.997855</td>\n",
              "      <td>0.987357</td>\n",
              "      <td>0.170581</td>\n",
              "      <td>0.506124</td>\n",
              "      <td>0.676764</td>\n",
              "      <td>0.822529</td>\n",
              "      <td>0.147741</td>\n",
              "      <td>-0.311631</td>\n",
              "      <td>0.679090</td>\n",
              "      <td>-0.097317</td>\n",
              "      <td>0.674172</td>\n",
              "      <td>0.735864</td>\n",
              "      <td>-0.222600</td>\n",
              "      <td>0.206000</td>\n",
              "      <td>0.194204</td>\n",
              "      <td>0.376169</td>\n",
              "      <td>-0.104321</td>\n",
              "      <td>-0.042641</td>\n",
              "      <td>0.969539</td>\n",
              "      <td>0.297008</td>\n",
              "      <td>0.965137</td>\n",
              "      <td>0.941082</td>\n",
              "      <td>0.119616</td>\n",
              "      <td>0.413463</td>\n",
              "      <td>0.526911</td>\n",
              "      <td>0.744214</td>\n",
              "      <td>0.163953</td>\n",
              "      <td>0.007066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_mean</th>\n",
              "      <td>0.099770</td>\n",
              "      <td>0.415185</td>\n",
              "      <td>0.323782</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.329533</td>\n",
              "      <td>0.321086</td>\n",
              "      <td>-0.023389</td>\n",
              "      <td>0.236702</td>\n",
              "      <td>0.302418</td>\n",
              "      <td>0.293464</td>\n",
              "      <td>0.071401</td>\n",
              "      <td>-0.076437</td>\n",
              "      <td>0.275869</td>\n",
              "      <td>0.386358</td>\n",
              "      <td>0.281673</td>\n",
              "      <td>0.259845</td>\n",
              "      <td>0.006614</td>\n",
              "      <td>0.191975</td>\n",
              "      <td>0.143293</td>\n",
              "      <td>0.163851</td>\n",
              "      <td>0.009127</td>\n",
              "      <td>0.054458</td>\n",
              "      <td>0.352573</td>\n",
              "      <td>0.912045</td>\n",
              "      <td>0.358040</td>\n",
              "      <td>0.343546</td>\n",
              "      <td>0.077503</td>\n",
              "      <td>0.277830</td>\n",
              "      <td>0.301025</td>\n",
              "      <td>0.295316</td>\n",
              "      <td>0.105008</td>\n",
              "      <td>0.119205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_mean</th>\n",
              "      <td>0.073159</td>\n",
              "      <td>0.742636</td>\n",
              "      <td>0.997855</td>\n",
              "      <td>0.329533</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.986507</td>\n",
              "      <td>0.207278</td>\n",
              "      <td>0.556936</td>\n",
              "      <td>0.716136</td>\n",
              "      <td>0.850977</td>\n",
              "      <td>0.183027</td>\n",
              "      <td>-0.261477</td>\n",
              "      <td>0.691765</td>\n",
              "      <td>-0.086761</td>\n",
              "      <td>0.693135</td>\n",
              "      <td>0.744983</td>\n",
              "      <td>-0.202694</td>\n",
              "      <td>0.250744</td>\n",
              "      <td>0.228082</td>\n",
              "      <td>0.407217</td>\n",
              "      <td>-0.081629</td>\n",
              "      <td>-0.005523</td>\n",
              "      <td>0.969476</td>\n",
              "      <td>0.303038</td>\n",
              "      <td>0.970387</td>\n",
              "      <td>0.941550</td>\n",
              "      <td>0.150549</td>\n",
              "      <td>0.455774</td>\n",
              "      <td>0.563879</td>\n",
              "      <td>0.771241</td>\n",
              "      <td>0.189115</td>\n",
              "      <td>0.051019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_mean</th>\n",
              "      <td>0.096893</td>\n",
              "      <td>0.708984</td>\n",
              "      <td>0.987357</td>\n",
              "      <td>0.321086</td>\n",
              "      <td>0.986507</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.177028</td>\n",
              "      <td>0.498502</td>\n",
              "      <td>0.685983</td>\n",
              "      <td>0.823269</td>\n",
              "      <td>0.151293</td>\n",
              "      <td>-0.283110</td>\n",
              "      <td>0.732562</td>\n",
              "      <td>-0.066280</td>\n",
              "      <td>0.726628</td>\n",
              "      <td>0.800086</td>\n",
              "      <td>-0.166777</td>\n",
              "      <td>0.212583</td>\n",
              "      <td>0.207660</td>\n",
              "      <td>0.372320</td>\n",
              "      <td>-0.072497</td>\n",
              "      <td>-0.019887</td>\n",
              "      <td>0.962746</td>\n",
              "      <td>0.287489</td>\n",
              "      <td>0.959120</td>\n",
              "      <td>0.959213</td>\n",
              "      <td>0.123523</td>\n",
              "      <td>0.390410</td>\n",
              "      <td>0.512606</td>\n",
              "      <td>0.722017</td>\n",
              "      <td>0.143570</td>\n",
              "      <td>0.003738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_mean</th>\n",
              "      <td>-0.012968</td>\n",
              "      <td>0.358560</td>\n",
              "      <td>0.170581</td>\n",
              "      <td>-0.023389</td>\n",
              "      <td>0.207278</td>\n",
              "      <td>0.177028</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.659123</td>\n",
              "      <td>0.521984</td>\n",
              "      <td>0.553695</td>\n",
              "      <td>0.557775</td>\n",
              "      <td>0.584792</td>\n",
              "      <td>0.301467</td>\n",
              "      <td>0.068406</td>\n",
              "      <td>0.296092</td>\n",
              "      <td>0.246552</td>\n",
              "      <td>0.332375</td>\n",
              "      <td>0.318943</td>\n",
              "      <td>0.248396</td>\n",
              "      <td>0.380676</td>\n",
              "      <td>0.200774</td>\n",
              "      <td>0.283607</td>\n",
              "      <td>0.213120</td>\n",
              "      <td>0.036072</td>\n",
              "      <td>0.238853</td>\n",
              "      <td>0.206718</td>\n",
              "      <td>0.805324</td>\n",
              "      <td>0.472468</td>\n",
              "      <td>0.434926</td>\n",
              "      <td>0.503053</td>\n",
              "      <td>0.394309</td>\n",
              "      <td>0.499316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_mean</th>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.596534</td>\n",
              "      <td>0.506124</td>\n",
              "      <td>0.236702</td>\n",
              "      <td>0.556936</td>\n",
              "      <td>0.498502</td>\n",
              "      <td>0.659123</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.883121</td>\n",
              "      <td>0.831135</td>\n",
              "      <td>0.602641</td>\n",
              "      <td>0.565369</td>\n",
              "      <td>0.497473</td>\n",
              "      <td>0.046205</td>\n",
              "      <td>0.548905</td>\n",
              "      <td>0.455653</td>\n",
              "      <td>0.135299</td>\n",
              "      <td>0.738722</td>\n",
              "      <td>0.570517</td>\n",
              "      <td>0.642262</td>\n",
              "      <td>0.229977</td>\n",
              "      <td>0.507318</td>\n",
              "      <td>0.535315</td>\n",
              "      <td>0.248133</td>\n",
              "      <td>0.590210</td>\n",
              "      <td>0.509604</td>\n",
              "      <td>0.565541</td>\n",
              "      <td>0.865809</td>\n",
              "      <td>0.816275</td>\n",
              "      <td>0.815573</td>\n",
              "      <td>0.510223</td>\n",
              "      <td>0.687382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_mean</th>\n",
              "      <td>0.050080</td>\n",
              "      <td>0.696360</td>\n",
              "      <td>0.676764</td>\n",
              "      <td>0.302418</td>\n",
              "      <td>0.716136</td>\n",
              "      <td>0.685983</td>\n",
              "      <td>0.521984</td>\n",
              "      <td>0.883121</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.921391</td>\n",
              "      <td>0.500667</td>\n",
              "      <td>0.336783</td>\n",
              "      <td>0.631925</td>\n",
              "      <td>0.076218</td>\n",
              "      <td>0.660391</td>\n",
              "      <td>0.617427</td>\n",
              "      <td>0.098564</td>\n",
              "      <td>0.670279</td>\n",
              "      <td>0.691270</td>\n",
              "      <td>0.683260</td>\n",
              "      <td>0.178009</td>\n",
              "      <td>0.449301</td>\n",
              "      <td>0.688236</td>\n",
              "      <td>0.299879</td>\n",
              "      <td>0.729565</td>\n",
              "      <td>0.675987</td>\n",
              "      <td>0.448822</td>\n",
              "      <td>0.754968</td>\n",
              "      <td>0.884103</td>\n",
              "      <td>0.861323</td>\n",
              "      <td>0.409464</td>\n",
              "      <td>0.514930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points_mean</th>\n",
              "      <td>0.044158</td>\n",
              "      <td>0.776614</td>\n",
              "      <td>0.822529</td>\n",
              "      <td>0.293464</td>\n",
              "      <td>0.850977</td>\n",
              "      <td>0.823269</td>\n",
              "      <td>0.553695</td>\n",
              "      <td>0.831135</td>\n",
              "      <td>0.921391</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.462497</td>\n",
              "      <td>0.166917</td>\n",
              "      <td>0.698050</td>\n",
              "      <td>0.021480</td>\n",
              "      <td>0.710650</td>\n",
              "      <td>0.690299</td>\n",
              "      <td>0.027653</td>\n",
              "      <td>0.490424</td>\n",
              "      <td>0.439167</td>\n",
              "      <td>0.615634</td>\n",
              "      <td>0.095351</td>\n",
              "      <td>0.257584</td>\n",
              "      <td>0.830318</td>\n",
              "      <td>0.292752</td>\n",
              "      <td>0.855923</td>\n",
              "      <td>0.809630</td>\n",
              "      <td>0.452753</td>\n",
              "      <td>0.667454</td>\n",
              "      <td>0.752399</td>\n",
              "      <td>0.910155</td>\n",
              "      <td>0.375744</td>\n",
              "      <td>0.368661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_mean</th>\n",
              "      <td>-0.022114</td>\n",
              "      <td>0.330499</td>\n",
              "      <td>0.147741</td>\n",
              "      <td>0.071401</td>\n",
              "      <td>0.183027</td>\n",
              "      <td>0.151293</td>\n",
              "      <td>0.557775</td>\n",
              "      <td>0.602641</td>\n",
              "      <td>0.500667</td>\n",
              "      <td>0.462497</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.479921</td>\n",
              "      <td>0.303379</td>\n",
              "      <td>0.128053</td>\n",
              "      <td>0.313893</td>\n",
              "      <td>0.223970</td>\n",
              "      <td>0.187321</td>\n",
              "      <td>0.421659</td>\n",
              "      <td>0.342627</td>\n",
              "      <td>0.393298</td>\n",
              "      <td>0.449137</td>\n",
              "      <td>0.331786</td>\n",
              "      <td>0.185728</td>\n",
              "      <td>0.090651</td>\n",
              "      <td>0.219169</td>\n",
              "      <td>0.177193</td>\n",
              "      <td>0.426675</td>\n",
              "      <td>0.473200</td>\n",
              "      <td>0.433721</td>\n",
              "      <td>0.430297</td>\n",
              "      <td>0.699826</td>\n",
              "      <td>0.438413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <td>-0.052511</td>\n",
              "      <td>-0.012838</td>\n",
              "      <td>-0.311631</td>\n",
              "      <td>-0.076437</td>\n",
              "      <td>-0.261477</td>\n",
              "      <td>-0.283110</td>\n",
              "      <td>0.584792</td>\n",
              "      <td>0.565369</td>\n",
              "      <td>0.336783</td>\n",
              "      <td>0.166917</td>\n",
              "      <td>0.479921</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.164174</td>\n",
              "      <td>0.039830</td>\n",
              "      <td>-0.090170</td>\n",
              "      <td>0.401964</td>\n",
              "      <td>0.559837</td>\n",
              "      <td>0.446630</td>\n",
              "      <td>0.341198</td>\n",
              "      <td>0.345007</td>\n",
              "      <td>0.688132</td>\n",
              "      <td>-0.253691</td>\n",
              "      <td>-0.051269</td>\n",
              "      <td>-0.205151</td>\n",
              "      <td>-0.231854</td>\n",
              "      <td>0.504942</td>\n",
              "      <td>0.458798</td>\n",
              "      <td>0.346234</td>\n",
              "      <td>0.175325</td>\n",
              "      <td>0.334019</td>\n",
              "      <td>0.767297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_se</th>\n",
              "      <td>0.143048</td>\n",
              "      <td>0.567134</td>\n",
              "      <td>0.679090</td>\n",
              "      <td>0.275869</td>\n",
              "      <td>0.691765</td>\n",
              "      <td>0.732562</td>\n",
              "      <td>0.301467</td>\n",
              "      <td>0.497473</td>\n",
              "      <td>0.631925</td>\n",
              "      <td>0.698050</td>\n",
              "      <td>0.303379</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>0.972794</td>\n",
              "      <td>0.951830</td>\n",
              "      <td>0.164514</td>\n",
              "      <td>0.356065</td>\n",
              "      <td>0.332358</td>\n",
              "      <td>0.513346</td>\n",
              "      <td>0.240567</td>\n",
              "      <td>0.227754</td>\n",
              "      <td>0.715065</td>\n",
              "      <td>0.194799</td>\n",
              "      <td>0.719684</td>\n",
              "      <td>0.751548</td>\n",
              "      <td>0.141919</td>\n",
              "      <td>0.287103</td>\n",
              "      <td>0.380585</td>\n",
              "      <td>0.531062</td>\n",
              "      <td>0.094543</td>\n",
              "      <td>0.049559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_se</th>\n",
              "      <td>-0.007526</td>\n",
              "      <td>-0.008303</td>\n",
              "      <td>-0.097317</td>\n",
              "      <td>0.386358</td>\n",
              "      <td>-0.086761</td>\n",
              "      <td>-0.066280</td>\n",
              "      <td>0.068406</td>\n",
              "      <td>0.046205</td>\n",
              "      <td>0.076218</td>\n",
              "      <td>0.021480</td>\n",
              "      <td>0.128053</td>\n",
              "      <td>0.164174</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.223171</td>\n",
              "      <td>0.111567</td>\n",
              "      <td>0.397243</td>\n",
              "      <td>0.231700</td>\n",
              "      <td>0.194998</td>\n",
              "      <td>0.230283</td>\n",
              "      <td>0.411621</td>\n",
              "      <td>0.279723</td>\n",
              "      <td>-0.111690</td>\n",
              "      <td>0.409003</td>\n",
              "      <td>-0.102242</td>\n",
              "      <td>-0.083195</td>\n",
              "      <td>-0.073658</td>\n",
              "      <td>-0.092439</td>\n",
              "      <td>-0.068956</td>\n",
              "      <td>-0.119638</td>\n",
              "      <td>-0.128215</td>\n",
              "      <td>-0.045655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_se</th>\n",
              "      <td>0.137331</td>\n",
              "      <td>0.556141</td>\n",
              "      <td>0.674172</td>\n",
              "      <td>0.281673</td>\n",
              "      <td>0.693135</td>\n",
              "      <td>0.726628</td>\n",
              "      <td>0.296092</td>\n",
              "      <td>0.548905</td>\n",
              "      <td>0.660391</td>\n",
              "      <td>0.710650</td>\n",
              "      <td>0.313893</td>\n",
              "      <td>0.039830</td>\n",
              "      <td>0.972794</td>\n",
              "      <td>0.223171</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.937655</td>\n",
              "      <td>0.151075</td>\n",
              "      <td>0.416322</td>\n",
              "      <td>0.362482</td>\n",
              "      <td>0.556264</td>\n",
              "      <td>0.266487</td>\n",
              "      <td>0.244143</td>\n",
              "      <td>0.697201</td>\n",
              "      <td>0.200371</td>\n",
              "      <td>0.721031</td>\n",
              "      <td>0.730713</td>\n",
              "      <td>0.130054</td>\n",
              "      <td>0.341919</td>\n",
              "      <td>0.418899</td>\n",
              "      <td>0.554897</td>\n",
              "      <td>0.109930</td>\n",
              "      <td>0.085433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_se</th>\n",
              "      <td>0.177742</td>\n",
              "      <td>0.548236</td>\n",
              "      <td>0.735864</td>\n",
              "      <td>0.259845</td>\n",
              "      <td>0.744983</td>\n",
              "      <td>0.800086</td>\n",
              "      <td>0.246552</td>\n",
              "      <td>0.455653</td>\n",
              "      <td>0.617427</td>\n",
              "      <td>0.690299</td>\n",
              "      <td>0.223970</td>\n",
              "      <td>-0.090170</td>\n",
              "      <td>0.951830</td>\n",
              "      <td>0.111567</td>\n",
              "      <td>0.937655</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.075150</td>\n",
              "      <td>0.284840</td>\n",
              "      <td>0.270895</td>\n",
              "      <td>0.415730</td>\n",
              "      <td>0.134109</td>\n",
              "      <td>0.127071</td>\n",
              "      <td>0.757373</td>\n",
              "      <td>0.196497</td>\n",
              "      <td>0.761213</td>\n",
              "      <td>0.811408</td>\n",
              "      <td>0.125389</td>\n",
              "      <td>0.283257</td>\n",
              "      <td>0.385100</td>\n",
              "      <td>0.538166</td>\n",
              "      <td>0.074126</td>\n",
              "      <td>0.017539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_se</th>\n",
              "      <td>0.096781</td>\n",
              "      <td>-0.067016</td>\n",
              "      <td>-0.222600</td>\n",
              "      <td>0.006614</td>\n",
              "      <td>-0.202694</td>\n",
              "      <td>-0.166777</td>\n",
              "      <td>0.332375</td>\n",
              "      <td>0.135299</td>\n",
              "      <td>0.098564</td>\n",
              "      <td>0.027653</td>\n",
              "      <td>0.187321</td>\n",
              "      <td>0.401964</td>\n",
              "      <td>0.164514</td>\n",
              "      <td>0.397243</td>\n",
              "      <td>0.151075</td>\n",
              "      <td>0.075150</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.336696</td>\n",
              "      <td>0.268685</td>\n",
              "      <td>0.328429</td>\n",
              "      <td>0.413506</td>\n",
              "      <td>0.427374</td>\n",
              "      <td>-0.230691</td>\n",
              "      <td>-0.074743</td>\n",
              "      <td>-0.217304</td>\n",
              "      <td>-0.182195</td>\n",
              "      <td>0.314457</td>\n",
              "      <td>-0.055558</td>\n",
              "      <td>-0.058298</td>\n",
              "      <td>-0.102007</td>\n",
              "      <td>-0.107342</td>\n",
              "      <td>0.101480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_se</th>\n",
              "      <td>0.033961</td>\n",
              "      <td>0.292999</td>\n",
              "      <td>0.206000</td>\n",
              "      <td>0.191975</td>\n",
              "      <td>0.250744</td>\n",
              "      <td>0.212583</td>\n",
              "      <td>0.318943</td>\n",
              "      <td>0.738722</td>\n",
              "      <td>0.670279</td>\n",
              "      <td>0.490424</td>\n",
              "      <td>0.421659</td>\n",
              "      <td>0.559837</td>\n",
              "      <td>0.356065</td>\n",
              "      <td>0.231700</td>\n",
              "      <td>0.416322</td>\n",
              "      <td>0.284840</td>\n",
              "      <td>0.336696</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.801268</td>\n",
              "      <td>0.744083</td>\n",
              "      <td>0.394713</td>\n",
              "      <td>0.803269</td>\n",
              "      <td>0.204607</td>\n",
              "      <td>0.143003</td>\n",
              "      <td>0.260516</td>\n",
              "      <td>0.199371</td>\n",
              "      <td>0.227394</td>\n",
              "      <td>0.678780</td>\n",
              "      <td>0.639147</td>\n",
              "      <td>0.483208</td>\n",
              "      <td>0.277878</td>\n",
              "      <td>0.590973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_se</th>\n",
              "      <td>0.055239</td>\n",
              "      <td>0.253730</td>\n",
              "      <td>0.194204</td>\n",
              "      <td>0.143293</td>\n",
              "      <td>0.228082</td>\n",
              "      <td>0.207660</td>\n",
              "      <td>0.248396</td>\n",
              "      <td>0.570517</td>\n",
              "      <td>0.691270</td>\n",
              "      <td>0.439167</td>\n",
              "      <td>0.342627</td>\n",
              "      <td>0.446630</td>\n",
              "      <td>0.332358</td>\n",
              "      <td>0.194998</td>\n",
              "      <td>0.362482</td>\n",
              "      <td>0.270895</td>\n",
              "      <td>0.268685</td>\n",
              "      <td>0.801268</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.771804</td>\n",
              "      <td>0.309429</td>\n",
              "      <td>0.727372</td>\n",
              "      <td>0.186904</td>\n",
              "      <td>0.100241</td>\n",
              "      <td>0.226680</td>\n",
              "      <td>0.188353</td>\n",
              "      <td>0.168481</td>\n",
              "      <td>0.484858</td>\n",
              "      <td>0.662564</td>\n",
              "      <td>0.440472</td>\n",
              "      <td>0.197788</td>\n",
              "      <td>0.439329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points_se</th>\n",
              "      <td>0.078768</td>\n",
              "      <td>0.408042</td>\n",
              "      <td>0.376169</td>\n",
              "      <td>0.163851</td>\n",
              "      <td>0.407217</td>\n",
              "      <td>0.372320</td>\n",
              "      <td>0.380676</td>\n",
              "      <td>0.642262</td>\n",
              "      <td>0.683260</td>\n",
              "      <td>0.615634</td>\n",
              "      <td>0.393298</td>\n",
              "      <td>0.341198</td>\n",
              "      <td>0.513346</td>\n",
              "      <td>0.230283</td>\n",
              "      <td>0.556264</td>\n",
              "      <td>0.415730</td>\n",
              "      <td>0.328429</td>\n",
              "      <td>0.744083</td>\n",
              "      <td>0.771804</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.312780</td>\n",
              "      <td>0.611044</td>\n",
              "      <td>0.358127</td>\n",
              "      <td>0.086741</td>\n",
              "      <td>0.394999</td>\n",
              "      <td>0.342271</td>\n",
              "      <td>0.215351</td>\n",
              "      <td>0.452888</td>\n",
              "      <td>0.549592</td>\n",
              "      <td>0.602450</td>\n",
              "      <td>0.143116</td>\n",
              "      <td>0.310655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_se</th>\n",
              "      <td>-0.017306</td>\n",
              "      <td>-0.006522</td>\n",
              "      <td>-0.104321</td>\n",
              "      <td>0.009127</td>\n",
              "      <td>-0.081629</td>\n",
              "      <td>-0.072497</td>\n",
              "      <td>0.200774</td>\n",
              "      <td>0.229977</td>\n",
              "      <td>0.178009</td>\n",
              "      <td>0.095351</td>\n",
              "      <td>0.449137</td>\n",
              "      <td>0.345007</td>\n",
              "      <td>0.240567</td>\n",
              "      <td>0.411621</td>\n",
              "      <td>0.266487</td>\n",
              "      <td>0.134109</td>\n",
              "      <td>0.413506</td>\n",
              "      <td>0.394713</td>\n",
              "      <td>0.309429</td>\n",
              "      <td>0.312780</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.369078</td>\n",
              "      <td>-0.128121</td>\n",
              "      <td>-0.077473</td>\n",
              "      <td>-0.103753</td>\n",
              "      <td>-0.110343</td>\n",
              "      <td>-0.012662</td>\n",
              "      <td>0.060255</td>\n",
              "      <td>0.037119</td>\n",
              "      <td>-0.030413</td>\n",
              "      <td>0.389402</td>\n",
              "      <td>0.078079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <td>0.025725</td>\n",
              "      <td>0.077972</td>\n",
              "      <td>-0.042641</td>\n",
              "      <td>0.054458</td>\n",
              "      <td>-0.005523</td>\n",
              "      <td>-0.019887</td>\n",
              "      <td>0.283607</td>\n",
              "      <td>0.507318</td>\n",
              "      <td>0.449301</td>\n",
              "      <td>0.257584</td>\n",
              "      <td>0.331786</td>\n",
              "      <td>0.688132</td>\n",
              "      <td>0.227754</td>\n",
              "      <td>0.279723</td>\n",
              "      <td>0.244143</td>\n",
              "      <td>0.127071</td>\n",
              "      <td>0.427374</td>\n",
              "      <td>0.803269</td>\n",
              "      <td>0.727372</td>\n",
              "      <td>0.611044</td>\n",
              "      <td>0.369078</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.037488</td>\n",
              "      <td>-0.003195</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>-0.022736</td>\n",
              "      <td>0.170568</td>\n",
              "      <td>0.390159</td>\n",
              "      <td>0.379975</td>\n",
              "      <td>0.215204</td>\n",
              "      <td>0.111094</td>\n",
              "      <td>0.591328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_worst</th>\n",
              "      <td>0.082405</td>\n",
              "      <td>0.776454</td>\n",
              "      <td>0.969539</td>\n",
              "      <td>0.352573</td>\n",
              "      <td>0.969476</td>\n",
              "      <td>0.962746</td>\n",
              "      <td>0.213120</td>\n",
              "      <td>0.535315</td>\n",
              "      <td>0.688236</td>\n",
              "      <td>0.830318</td>\n",
              "      <td>0.185728</td>\n",
              "      <td>-0.253691</td>\n",
              "      <td>0.715065</td>\n",
              "      <td>-0.111690</td>\n",
              "      <td>0.697201</td>\n",
              "      <td>0.757373</td>\n",
              "      <td>-0.230691</td>\n",
              "      <td>0.204607</td>\n",
              "      <td>0.186904</td>\n",
              "      <td>0.358127</td>\n",
              "      <td>-0.128121</td>\n",
              "      <td>-0.037488</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.359921</td>\n",
              "      <td>0.993708</td>\n",
              "      <td>0.984015</td>\n",
              "      <td>0.216574</td>\n",
              "      <td>0.475820</td>\n",
              "      <td>0.573975</td>\n",
              "      <td>0.787424</td>\n",
              "      <td>0.243529</td>\n",
              "      <td>0.093492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_worst</th>\n",
              "      <td>0.064720</td>\n",
              "      <td>0.456903</td>\n",
              "      <td>0.297008</td>\n",
              "      <td>0.912045</td>\n",
              "      <td>0.303038</td>\n",
              "      <td>0.287489</td>\n",
              "      <td>0.036072</td>\n",
              "      <td>0.248133</td>\n",
              "      <td>0.299879</td>\n",
              "      <td>0.292752</td>\n",
              "      <td>0.090651</td>\n",
              "      <td>-0.051269</td>\n",
              "      <td>0.194799</td>\n",
              "      <td>0.409003</td>\n",
              "      <td>0.200371</td>\n",
              "      <td>0.196497</td>\n",
              "      <td>-0.074743</td>\n",
              "      <td>0.143003</td>\n",
              "      <td>0.100241</td>\n",
              "      <td>0.086741</td>\n",
              "      <td>-0.077473</td>\n",
              "      <td>-0.003195</td>\n",
              "      <td>0.359921</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.365098</td>\n",
              "      <td>0.345842</td>\n",
              "      <td>0.225429</td>\n",
              "      <td>0.360832</td>\n",
              "      <td>0.368366</td>\n",
              "      <td>0.359755</td>\n",
              "      <td>0.233027</td>\n",
              "      <td>0.219122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_worst</th>\n",
              "      <td>0.079986</td>\n",
              "      <td>0.782914</td>\n",
              "      <td>0.965137</td>\n",
              "      <td>0.358040</td>\n",
              "      <td>0.970387</td>\n",
              "      <td>0.959120</td>\n",
              "      <td>0.238853</td>\n",
              "      <td>0.590210</td>\n",
              "      <td>0.729565</td>\n",
              "      <td>0.855923</td>\n",
              "      <td>0.219169</td>\n",
              "      <td>-0.205151</td>\n",
              "      <td>0.719684</td>\n",
              "      <td>-0.102242</td>\n",
              "      <td>0.721031</td>\n",
              "      <td>0.761213</td>\n",
              "      <td>-0.217304</td>\n",
              "      <td>0.260516</td>\n",
              "      <td>0.226680</td>\n",
              "      <td>0.394999</td>\n",
              "      <td>-0.103753</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.993708</td>\n",
              "      <td>0.365098</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.977578</td>\n",
              "      <td>0.236775</td>\n",
              "      <td>0.529408</td>\n",
              "      <td>0.618344</td>\n",
              "      <td>0.816322</td>\n",
              "      <td>0.269493</td>\n",
              "      <td>0.138957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_worst</th>\n",
              "      <td>0.107187</td>\n",
              "      <td>0.733825</td>\n",
              "      <td>0.941082</td>\n",
              "      <td>0.343546</td>\n",
              "      <td>0.941550</td>\n",
              "      <td>0.959213</td>\n",
              "      <td>0.206718</td>\n",
              "      <td>0.509604</td>\n",
              "      <td>0.675987</td>\n",
              "      <td>0.809630</td>\n",
              "      <td>0.177193</td>\n",
              "      <td>-0.231854</td>\n",
              "      <td>0.751548</td>\n",
              "      <td>-0.083195</td>\n",
              "      <td>0.730713</td>\n",
              "      <td>0.811408</td>\n",
              "      <td>-0.182195</td>\n",
              "      <td>0.199371</td>\n",
              "      <td>0.188353</td>\n",
              "      <td>0.342271</td>\n",
              "      <td>-0.110343</td>\n",
              "      <td>-0.022736</td>\n",
              "      <td>0.984015</td>\n",
              "      <td>0.345842</td>\n",
              "      <td>0.977578</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.209145</td>\n",
              "      <td>0.438296</td>\n",
              "      <td>0.543331</td>\n",
              "      <td>0.747419</td>\n",
              "      <td>0.209146</td>\n",
              "      <td>0.079647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_worst</th>\n",
              "      <td>0.010338</td>\n",
              "      <td>0.421465</td>\n",
              "      <td>0.119616</td>\n",
              "      <td>0.077503</td>\n",
              "      <td>0.150549</td>\n",
              "      <td>0.123523</td>\n",
              "      <td>0.805324</td>\n",
              "      <td>0.565541</td>\n",
              "      <td>0.448822</td>\n",
              "      <td>0.452753</td>\n",
              "      <td>0.426675</td>\n",
              "      <td>0.504942</td>\n",
              "      <td>0.141919</td>\n",
              "      <td>-0.073658</td>\n",
              "      <td>0.130054</td>\n",
              "      <td>0.125389</td>\n",
              "      <td>0.314457</td>\n",
              "      <td>0.227394</td>\n",
              "      <td>0.168481</td>\n",
              "      <td>0.215351</td>\n",
              "      <td>-0.012662</td>\n",
              "      <td>0.170568</td>\n",
              "      <td>0.216574</td>\n",
              "      <td>0.225429</td>\n",
              "      <td>0.236775</td>\n",
              "      <td>0.209145</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.568187</td>\n",
              "      <td>0.518523</td>\n",
              "      <td>0.547691</td>\n",
              "      <td>0.493838</td>\n",
              "      <td>0.617624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_worst</th>\n",
              "      <td>-0.002968</td>\n",
              "      <td>0.590998</td>\n",
              "      <td>0.413463</td>\n",
              "      <td>0.277830</td>\n",
              "      <td>0.455774</td>\n",
              "      <td>0.390410</td>\n",
              "      <td>0.472468</td>\n",
              "      <td>0.865809</td>\n",
              "      <td>0.754968</td>\n",
              "      <td>0.667454</td>\n",
              "      <td>0.473200</td>\n",
              "      <td>0.458798</td>\n",
              "      <td>0.287103</td>\n",
              "      <td>-0.092439</td>\n",
              "      <td>0.341919</td>\n",
              "      <td>0.283257</td>\n",
              "      <td>-0.055558</td>\n",
              "      <td>0.678780</td>\n",
              "      <td>0.484858</td>\n",
              "      <td>0.452888</td>\n",
              "      <td>0.060255</td>\n",
              "      <td>0.390159</td>\n",
              "      <td>0.475820</td>\n",
              "      <td>0.360832</td>\n",
              "      <td>0.529408</td>\n",
              "      <td>0.438296</td>\n",
              "      <td>0.568187</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.892261</td>\n",
              "      <td>0.801080</td>\n",
              "      <td>0.614441</td>\n",
              "      <td>0.810455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_worst</th>\n",
              "      <td>0.023203</td>\n",
              "      <td>0.659610</td>\n",
              "      <td>0.526911</td>\n",
              "      <td>0.301025</td>\n",
              "      <td>0.563879</td>\n",
              "      <td>0.512606</td>\n",
              "      <td>0.434926</td>\n",
              "      <td>0.816275</td>\n",
              "      <td>0.884103</td>\n",
              "      <td>0.752399</td>\n",
              "      <td>0.433721</td>\n",
              "      <td>0.346234</td>\n",
              "      <td>0.380585</td>\n",
              "      <td>-0.068956</td>\n",
              "      <td>0.418899</td>\n",
              "      <td>0.385100</td>\n",
              "      <td>-0.058298</td>\n",
              "      <td>0.639147</td>\n",
              "      <td>0.662564</td>\n",
              "      <td>0.549592</td>\n",
              "      <td>0.037119</td>\n",
              "      <td>0.379975</td>\n",
              "      <td>0.573975</td>\n",
              "      <td>0.368366</td>\n",
              "      <td>0.618344</td>\n",
              "      <td>0.543331</td>\n",
              "      <td>0.518523</td>\n",
              "      <td>0.892261</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.855434</td>\n",
              "      <td>0.532520</td>\n",
              "      <td>0.686511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points_worst</th>\n",
              "      <td>0.035174</td>\n",
              "      <td>0.793566</td>\n",
              "      <td>0.744214</td>\n",
              "      <td>0.295316</td>\n",
              "      <td>0.771241</td>\n",
              "      <td>0.722017</td>\n",
              "      <td>0.503053</td>\n",
              "      <td>0.815573</td>\n",
              "      <td>0.861323</td>\n",
              "      <td>0.910155</td>\n",
              "      <td>0.430297</td>\n",
              "      <td>0.175325</td>\n",
              "      <td>0.531062</td>\n",
              "      <td>-0.119638</td>\n",
              "      <td>0.554897</td>\n",
              "      <td>0.538166</td>\n",
              "      <td>-0.102007</td>\n",
              "      <td>0.483208</td>\n",
              "      <td>0.440472</td>\n",
              "      <td>0.602450</td>\n",
              "      <td>-0.030413</td>\n",
              "      <td>0.215204</td>\n",
              "      <td>0.787424</td>\n",
              "      <td>0.359755</td>\n",
              "      <td>0.816322</td>\n",
              "      <td>0.747419</td>\n",
              "      <td>0.547691</td>\n",
              "      <td>0.801080</td>\n",
              "      <td>0.855434</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.502528</td>\n",
              "      <td>0.511114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_worst</th>\n",
              "      <td>-0.044224</td>\n",
              "      <td>0.416294</td>\n",
              "      <td>0.163953</td>\n",
              "      <td>0.105008</td>\n",
              "      <td>0.189115</td>\n",
              "      <td>0.143570</td>\n",
              "      <td>0.394309</td>\n",
              "      <td>0.510223</td>\n",
              "      <td>0.409464</td>\n",
              "      <td>0.375744</td>\n",
              "      <td>0.699826</td>\n",
              "      <td>0.334019</td>\n",
              "      <td>0.094543</td>\n",
              "      <td>-0.128215</td>\n",
              "      <td>0.109930</td>\n",
              "      <td>0.074126</td>\n",
              "      <td>-0.107342</td>\n",
              "      <td>0.277878</td>\n",
              "      <td>0.197788</td>\n",
              "      <td>0.143116</td>\n",
              "      <td>0.389402</td>\n",
              "      <td>0.111094</td>\n",
              "      <td>0.243529</td>\n",
              "      <td>0.233027</td>\n",
              "      <td>0.269493</td>\n",
              "      <td>0.209146</td>\n",
              "      <td>0.493838</td>\n",
              "      <td>0.614441</td>\n",
              "      <td>0.532520</td>\n",
              "      <td>0.502528</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.537848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <td>-0.029866</td>\n",
              "      <td>0.323872</td>\n",
              "      <td>0.007066</td>\n",
              "      <td>0.119205</td>\n",
              "      <td>0.051019</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>0.499316</td>\n",
              "      <td>0.687382</td>\n",
              "      <td>0.514930</td>\n",
              "      <td>0.368661</td>\n",
              "      <td>0.438413</td>\n",
              "      <td>0.767297</td>\n",
              "      <td>0.049559</td>\n",
              "      <td>-0.045655</td>\n",
              "      <td>0.085433</td>\n",
              "      <td>0.017539</td>\n",
              "      <td>0.101480</td>\n",
              "      <td>0.590973</td>\n",
              "      <td>0.439329</td>\n",
              "      <td>0.310655</td>\n",
              "      <td>0.078079</td>\n",
              "      <td>0.591328</td>\n",
              "      <td>0.093492</td>\n",
              "      <td>0.219122</td>\n",
              "      <td>0.138957</td>\n",
              "      <td>0.079647</td>\n",
              "      <td>0.617624</td>\n",
              "      <td>0.810455</td>\n",
              "      <td>0.686511</td>\n",
              "      <td>0.511114</td>\n",
              "      <td>0.537848</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               id  ...  fractal_dimension_worst\n",
              "id                       1.000000  ...                -0.029866\n",
              "diagnosis                0.039769  ...                 0.323872\n",
              "radius_mean              0.074626  ...                 0.007066\n",
              "texture_mean             0.099770  ...                 0.119205\n",
              "perimeter_mean           0.073159  ...                 0.051019\n",
              "area_mean                0.096893  ...                 0.003738\n",
              "smoothness_mean         -0.012968  ...                 0.499316\n",
              "compactness_mean         0.000096  ...                 0.687382\n",
              "concavity_mean           0.050080  ...                 0.514930\n",
              "concave points_mean      0.044158  ...                 0.368661\n",
              "symmetry_mean           -0.022114  ...                 0.438413\n",
              "fractal_dimension_mean  -0.052511  ...                 0.767297\n",
              "radius_se                0.143048  ...                 0.049559\n",
              "texture_se              -0.007526  ...                -0.045655\n",
              "perimeter_se             0.137331  ...                 0.085433\n",
              "area_se                  0.177742  ...                 0.017539\n",
              "smoothness_se            0.096781  ...                 0.101480\n",
              "compactness_se           0.033961  ...                 0.590973\n",
              "concavity_se             0.055239  ...                 0.439329\n",
              "concave points_se        0.078768  ...                 0.310655\n",
              "symmetry_se             -0.017306  ...                 0.078079\n",
              "fractal_dimension_se     0.025725  ...                 0.591328\n",
              "radius_worst             0.082405  ...                 0.093492\n",
              "texture_worst            0.064720  ...                 0.219122\n",
              "perimeter_worst          0.079986  ...                 0.138957\n",
              "area_worst               0.107187  ...                 0.079647\n",
              "smoothness_worst         0.010338  ...                 0.617624\n",
              "compactness_worst       -0.002968  ...                 0.810455\n",
              "concavity_worst          0.023203  ...                 0.686511\n",
              "concave points_worst     0.035174  ...                 0.511114\n",
              "symmetry_worst          -0.044224  ...                 0.537848\n",
              "fractal_dimension_worst -0.029866  ...                 1.000000\n",
              "\n",
              "[32 rows x 32 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvPqVOG9tiln"
      },
      "source": [
        "info.drop(['fractal_dimension_mean','texture_se','smoothness_se','symmetry_se','fractal_dimension_se'],axis=1,inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XYMb5lrvpNR"
      },
      "source": [
        "X=info.iloc[:,2:]\n",
        "y=info.diagnosis"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "mn4nB1mIagbU",
        "outputId": "e6713fa9-00db-4e24-9d94-2ab5617ad3f3"
      },
      "source": [
        "X"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows Ã— 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0          17.99         10.38  ...          0.4601                  0.11890\n",
              "1          20.57         17.77  ...          0.2750                  0.08902\n",
              "2          19.69         21.25  ...          0.3613                  0.08758\n",
              "3          11.42         20.38  ...          0.6638                  0.17300\n",
              "4          20.29         14.34  ...          0.2364                  0.07678\n",
              "..           ...           ...  ...             ...                      ...\n",
              "564        21.56         22.39  ...          0.2060                  0.07115\n",
              "565        20.13         28.25  ...          0.2572                  0.06637\n",
              "566        16.60         28.08  ...          0.2218                  0.07820\n",
              "567        20.60         29.33  ...          0.4087                  0.12400\n",
              "568         7.76         24.54  ...          0.2871                  0.07039\n",
              "\n",
              "[569 rows x 25 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L19yrjkRaguI",
        "outputId": "3c3273b8-ff4d-464e-a9ac-7bedc07783c3"
      },
      "source": [
        "y"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      1\n",
              "4      1\n",
              "      ..\n",
              "564    1\n",
              "565    1\n",
              "566    1\n",
              "567    1\n",
              "568    0\n",
              "Name: diagnosis, Length: 569, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoJ1U7I2vpNS"
      },
      "source": [
        "# split train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQez7UtUvpNS",
        "outputId": "a7bdab31-1509-423d-a094-688b4bd1a033"
      },
      "source": [
        "len(X_train),len(X_test),len(y_train),len(y_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(455, 114, 455, 114)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5uKoIVFvpNT"
      },
      "source": [
        "X_tr = np.array(X_train).reshape(X_train.shape[0], X_train.shape[1], 1).astype('float32')\n",
        "X_vd = np.array(X_test).reshape(X_test.shape[0], X_test.shape[1], 1).astype('float32')\n",
        "\n",
        "y_tr = np.array(y_train)\n",
        "y_vd = np.array(y_test)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lvE0XUKvpNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c48fefd-bc44-4857-ab51-6d37458b6571"
      },
      "source": [
        "X_tr.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(455, 25, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81VwpAT1vpNV"
      },
      "source": [
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, 5, input_shape = X_tr.shape[1:3]))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Conv1D(64, 1))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, input_dim=25))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Dense(32))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtWB9ANFvpNW",
        "outputId": "62819fd2-38a5-4bcb-9c06-86739aca2abd"
      },
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 21, 32)            192       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 21, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 21, 64)            2112      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1344)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               172160    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 178,625\n",
            "Trainable params: 178,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LAZbS6Jt6hI"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCMygyxYt6R6"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(10, input_dim=25, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLmInV-Dt9Qo",
        "outputId": "f650518d-cee0-4cf9-d855-773a46506cbb"
      },
      "source": [
        "m=model.fit(X_train,y_train, epochs=600, batch_size=5, validation_split=0.2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "73/73 [==============================] - 4s 5ms/step - loss: 11.1292 - accuracy: 0.3104 - val_loss: 1.1170 - val_accuracy: 0.7253\n",
            "Epoch 2/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.9020 - accuracy: 0.7115 - val_loss: 0.6798 - val_accuracy: 0.7033\n",
            "Epoch 3/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.8324 - accuracy: 0.7610 - val_loss: 0.3751 - val_accuracy: 0.8242\n",
            "Epoch 4/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.5900 - accuracy: 0.8379 - val_loss: 0.3183 - val_accuracy: 0.9231\n",
            "Epoch 5/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.5063 - accuracy: 0.8434 - val_loss: 0.2679 - val_accuracy: 0.9231\n",
            "Epoch 6/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.4487 - accuracy: 0.8874 - val_loss: 0.2520 - val_accuracy: 0.9341\n",
            "Epoch 7/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.4881 - accuracy: 0.8516 - val_loss: 0.2382 - val_accuracy: 0.9341\n",
            "Epoch 8/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8874 - val_loss: 0.2272 - val_accuracy: 0.9121\n",
            "Epoch 9/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3669 - accuracy: 0.8764 - val_loss: 0.2313 - val_accuracy: 0.9341\n",
            "Epoch 10/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3198 - accuracy: 0.8791 - val_loss: 0.2403 - val_accuracy: 0.9341\n",
            "Epoch 11/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3790 - accuracy: 0.8819 - val_loss: 0.3613 - val_accuracy: 0.8571\n",
            "Epoch 12/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.9038 - val_loss: 0.2316 - val_accuracy: 0.9011\n",
            "Epoch 13/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2944 - accuracy: 0.8984 - val_loss: 0.2308 - val_accuracy: 0.9231\n",
            "Epoch 14/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3876 - accuracy: 0.8819 - val_loss: 0.1893 - val_accuracy: 0.9231\n",
            "Epoch 15/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2969 - accuracy: 0.9011 - val_loss: 0.1881 - val_accuracy: 0.9231\n",
            "Epoch 16/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.8956 - val_loss: 0.2113 - val_accuracy: 0.9121\n",
            "Epoch 17/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3415 - accuracy: 0.8956 - val_loss: 0.3433 - val_accuracy: 0.9121\n",
            "Epoch 18/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3062 - accuracy: 0.9038 - val_loss: 0.1847 - val_accuracy: 0.9231\n",
            "Epoch 19/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2608 - accuracy: 0.9148 - val_loss: 0.2015 - val_accuracy: 0.9121\n",
            "Epoch 20/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2973 - accuracy: 0.8956 - val_loss: 0.2088 - val_accuracy: 0.9121\n",
            "Epoch 21/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2655 - accuracy: 0.9148 - val_loss: 0.2293 - val_accuracy: 0.9231\n",
            "Epoch 22/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3270 - accuracy: 0.8846 - val_loss: 0.2367 - val_accuracy: 0.9341\n",
            "Epoch 23/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2781 - accuracy: 0.9203 - val_loss: 0.2591 - val_accuracy: 0.9231\n",
            "Epoch 24/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2527 - accuracy: 0.9121 - val_loss: 0.2663 - val_accuracy: 0.9011\n",
            "Epoch 25/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8764 - val_loss: 0.1669 - val_accuracy: 0.9341\n",
            "Epoch 26/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2420 - accuracy: 0.9148 - val_loss: 0.1691 - val_accuracy: 0.9341\n",
            "Epoch 27/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2550 - accuracy: 0.9176 - val_loss: 0.1917 - val_accuracy: 0.9341\n",
            "Epoch 28/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2492 - accuracy: 0.9011 - val_loss: 0.2519 - val_accuracy: 0.9231\n",
            "Epoch 29/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.8984 - val_loss: 0.2870 - val_accuracy: 0.8791\n",
            "Epoch 30/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2460 - accuracy: 0.9093 - val_loss: 0.1757 - val_accuracy: 0.9341\n",
            "Epoch 31/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2942 - accuracy: 0.8956 - val_loss: 0.2440 - val_accuracy: 0.9231\n",
            "Epoch 32/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2663 - accuracy: 0.8956 - val_loss: 0.1613 - val_accuracy: 0.9341\n",
            "Epoch 33/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.8956 - val_loss: 0.2570 - val_accuracy: 0.9231\n",
            "Epoch 34/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2580 - accuracy: 0.9176 - val_loss: 0.1600 - val_accuracy: 0.9341\n",
            "Epoch 35/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2451 - accuracy: 0.9176 - val_loss: 0.1599 - val_accuracy: 0.9341\n",
            "Epoch 36/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2389 - accuracy: 0.9066 - val_loss: 0.1999 - val_accuracy: 0.9341\n",
            "Epoch 37/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.9148 - val_loss: 0.1581 - val_accuracy: 0.9341\n",
            "Epoch 38/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2234 - accuracy: 0.9203 - val_loss: 0.2001 - val_accuracy: 0.9341\n",
            "Epoch 39/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2457 - accuracy: 0.9093 - val_loss: 0.2247 - val_accuracy: 0.9231\n",
            "Epoch 40/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2357 - accuracy: 0.9148 - val_loss: 0.1639 - val_accuracy: 0.9341\n",
            "Epoch 41/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.8956 - val_loss: 0.1662 - val_accuracy: 0.9341\n",
            "Epoch 42/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.2587 - accuracy: 0.9121 - val_loss: 0.2173 - val_accuracy: 0.9231\n",
            "Epoch 43/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.9066 - val_loss: 0.1644 - val_accuracy: 0.9231\n",
            "Epoch 44/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3908 - accuracy: 0.8791 - val_loss: 0.3674 - val_accuracy: 0.8791\n",
            "Epoch 45/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3565 - accuracy: 0.8901 - val_loss: 0.1540 - val_accuracy: 0.9231\n",
            "Epoch 46/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2232 - accuracy: 0.9176 - val_loss: 0.1782 - val_accuracy: 0.9341\n",
            "Epoch 47/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2210 - accuracy: 0.9121 - val_loss: 0.1692 - val_accuracy: 0.9341\n",
            "Epoch 48/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.9231 - val_loss: 0.1734 - val_accuracy: 0.9341\n",
            "Epoch 49/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2428 - accuracy: 0.9011 - val_loss: 0.1456 - val_accuracy: 0.9341\n",
            "Epoch 50/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2219 - accuracy: 0.9066 - val_loss: 0.1503 - val_accuracy: 0.9341\n",
            "Epoch 51/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2192 - accuracy: 0.9176 - val_loss: 0.2047 - val_accuracy: 0.9231\n",
            "Epoch 52/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1924 - accuracy: 0.9286 - val_loss: 0.1772 - val_accuracy: 0.9560\n",
            "Epoch 53/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2214 - accuracy: 0.9148 - val_loss: 0.1484 - val_accuracy: 0.9231\n",
            "Epoch 54/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2210 - accuracy: 0.9176 - val_loss: 0.2250 - val_accuracy: 0.9231\n",
            "Epoch 55/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2425 - accuracy: 0.8984 - val_loss: 0.1826 - val_accuracy: 0.9341\n",
            "Epoch 56/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.9231 - val_loss: 0.1865 - val_accuracy: 0.9341\n",
            "Epoch 57/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2672 - accuracy: 0.9066 - val_loss: 0.2712 - val_accuracy: 0.9011\n",
            "Epoch 58/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2338 - accuracy: 0.9121 - val_loss: 0.1502 - val_accuracy: 0.9341\n",
            "Epoch 59/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2301 - accuracy: 0.9148 - val_loss: 0.1395 - val_accuracy: 0.9341\n",
            "Epoch 60/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1951 - accuracy: 0.9258 - val_loss: 0.1873 - val_accuracy: 0.9341\n",
            "Epoch 61/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.9093 - val_loss: 0.1388 - val_accuracy: 0.9341\n",
            "Epoch 62/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1910 - accuracy: 0.9258 - val_loss: 0.2145 - val_accuracy: 0.9231\n",
            "Epoch 63/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2017 - accuracy: 0.9066 - val_loss: 0.1423 - val_accuracy: 0.9341\n",
            "Epoch 64/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2587 - accuracy: 0.9121 - val_loss: 0.1310 - val_accuracy: 0.9341\n",
            "Epoch 65/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2492 - accuracy: 0.9121 - val_loss: 0.1975 - val_accuracy: 0.9231\n",
            "Epoch 66/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2171 - accuracy: 0.9121 - val_loss: 0.1341 - val_accuracy: 0.9341\n",
            "Epoch 67/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3045 - accuracy: 0.9038 - val_loss: 0.1873 - val_accuracy: 0.9231\n",
            "Epoch 68/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2389 - accuracy: 0.9121 - val_loss: 0.2075 - val_accuracy: 0.9231\n",
            "Epoch 69/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9341 - val_loss: 0.1789 - val_accuracy: 0.9341\n",
            "Epoch 70/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2065 - accuracy: 0.9066 - val_loss: 0.1330 - val_accuracy: 0.9341\n",
            "Epoch 71/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1988 - accuracy: 0.9176 - val_loss: 0.1403 - val_accuracy: 0.9451\n",
            "Epoch 72/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2012 - accuracy: 0.9286 - val_loss: 0.1319 - val_accuracy: 0.9341\n",
            "Epoch 73/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2163 - accuracy: 0.9258 - val_loss: 0.1367 - val_accuracy: 0.9341\n",
            "Epoch 74/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.9038 - val_loss: 0.1533 - val_accuracy: 0.9451\n",
            "Epoch 75/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2711 - accuracy: 0.8956 - val_loss: 0.3251 - val_accuracy: 0.9121\n",
            "Epoch 76/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9148 - val_loss: 0.2337 - val_accuracy: 0.8901\n",
            "Epoch 77/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2078 - accuracy: 0.9203 - val_loss: 0.1303 - val_accuracy: 0.9341\n",
            "Epoch 78/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2018 - accuracy: 0.9176 - val_loss: 0.1429 - val_accuracy: 0.9451\n",
            "Epoch 79/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2036 - accuracy: 0.9121 - val_loss: 0.1311 - val_accuracy: 0.9341\n",
            "Epoch 80/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1780 - accuracy: 0.9368 - val_loss: 0.1274 - val_accuracy: 0.9341\n",
            "Epoch 81/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2173 - accuracy: 0.9066 - val_loss: 0.2893 - val_accuracy: 0.9121\n",
            "Epoch 82/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2151 - accuracy: 0.9121 - val_loss: 0.1240 - val_accuracy: 0.9341\n",
            "Epoch 83/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1871 - accuracy: 0.9203 - val_loss: 0.2081 - val_accuracy: 0.9231\n",
            "Epoch 84/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2412 - accuracy: 0.9093 - val_loss: 0.1348 - val_accuracy: 0.9451\n",
            "Epoch 85/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2026 - accuracy: 0.9148 - val_loss: 0.1528 - val_accuracy: 0.9341\n",
            "Epoch 86/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1898 - accuracy: 0.9313 - val_loss: 0.1209 - val_accuracy: 0.9560\n",
            "Epoch 87/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2026 - accuracy: 0.9121 - val_loss: 0.1389 - val_accuracy: 0.9560\n",
            "Epoch 88/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1875 - accuracy: 0.9341 - val_loss: 0.2054 - val_accuracy: 0.9231\n",
            "Epoch 89/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2246 - accuracy: 0.9066 - val_loss: 0.2051 - val_accuracy: 0.9231\n",
            "Epoch 90/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1708 - accuracy: 0.9341 - val_loss: 0.1545 - val_accuracy: 0.9341\n",
            "Epoch 91/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.2123 - accuracy: 0.9203 - val_loss: 0.2727 - val_accuracy: 0.8791\n",
            "Epoch 92/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.3215 - accuracy: 0.8929 - val_loss: 0.1260 - val_accuracy: 0.9451\n",
            "Epoch 93/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2260 - accuracy: 0.9093 - val_loss: 0.1820 - val_accuracy: 0.9451\n",
            "Epoch 94/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.9176 - val_loss: 0.2131 - val_accuracy: 0.9231\n",
            "Epoch 95/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.9148 - val_loss: 0.1334 - val_accuracy: 0.9560\n",
            "Epoch 96/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1741 - accuracy: 0.9258 - val_loss: 0.1237 - val_accuracy: 0.9451\n",
            "Epoch 97/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1778 - accuracy: 0.9286 - val_loss: 0.1201 - val_accuracy: 0.9560\n",
            "Epoch 98/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.9258 - val_loss: 0.1773 - val_accuracy: 0.9231\n",
            "Epoch 99/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2309 - accuracy: 0.9011 - val_loss: 0.1419 - val_accuracy: 0.9560\n",
            "Epoch 100/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2084 - accuracy: 0.9176 - val_loss: 0.1477 - val_accuracy: 0.9560\n",
            "Epoch 101/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1900 - accuracy: 0.9203 - val_loss: 0.2881 - val_accuracy: 0.8681\n",
            "Epoch 102/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9286 - val_loss: 0.1737 - val_accuracy: 0.9451\n",
            "Epoch 103/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1804 - accuracy: 0.9203 - val_loss: 0.1720 - val_accuracy: 0.9231\n",
            "Epoch 104/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1892 - accuracy: 0.9341 - val_loss: 0.1599 - val_accuracy: 0.9451\n",
            "Epoch 105/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.9258 - val_loss: 0.1372 - val_accuracy: 0.9560\n",
            "Epoch 106/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1792 - accuracy: 0.9341 - val_loss: 0.1372 - val_accuracy: 0.9560\n",
            "Epoch 107/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9148 - val_loss: 0.1278 - val_accuracy: 0.9560\n",
            "Epoch 108/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1753 - accuracy: 0.9313 - val_loss: 0.1192 - val_accuracy: 0.9451\n",
            "Epoch 109/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1955 - accuracy: 0.9176 - val_loss: 0.2350 - val_accuracy: 0.9011\n",
            "Epoch 110/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1736 - accuracy: 0.9396 - val_loss: 0.1747 - val_accuracy: 0.9231\n",
            "Epoch 111/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9231 - val_loss: 0.1252 - val_accuracy: 0.9560\n",
            "Epoch 112/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1581 - accuracy: 0.9368 - val_loss: 0.1565 - val_accuracy: 0.9451\n",
            "Epoch 113/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1674 - accuracy: 0.9368 - val_loss: 0.1444 - val_accuracy: 0.9560\n",
            "Epoch 114/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1660 - accuracy: 0.9341 - val_loss: 0.1360 - val_accuracy: 0.9560\n",
            "Epoch 115/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1932 - accuracy: 0.9121 - val_loss: 0.1569 - val_accuracy: 0.9451\n",
            "Epoch 116/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1967 - accuracy: 0.9396 - val_loss: 0.1168 - val_accuracy: 0.9560\n",
            "Epoch 117/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1773 - accuracy: 0.9286 - val_loss: 0.1371 - val_accuracy: 0.9341\n",
            "Epoch 118/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1671 - accuracy: 0.9286 - val_loss: 0.1200 - val_accuracy: 0.9560\n",
            "Epoch 119/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1860 - accuracy: 0.9231 - val_loss: 0.1316 - val_accuracy: 0.9560\n",
            "Epoch 120/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1786 - accuracy: 0.9368 - val_loss: 0.1305 - val_accuracy: 0.9451\n",
            "Epoch 121/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.2235 - accuracy: 0.8984 - val_loss: 0.2081 - val_accuracy: 0.9341\n",
            "Epoch 122/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1607 - accuracy: 0.9396 - val_loss: 0.1383 - val_accuracy: 0.9341\n",
            "Epoch 123/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1590 - accuracy: 0.9313 - val_loss: 0.1436 - val_accuracy: 0.9560\n",
            "Epoch 124/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1618 - accuracy: 0.9176 - val_loss: 0.1269 - val_accuracy: 0.9560\n",
            "Epoch 125/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1519 - accuracy: 0.9451 - val_loss: 0.2974 - val_accuracy: 0.8462\n",
            "Epoch 126/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.9341 - val_loss: 0.1751 - val_accuracy: 0.9231\n",
            "Epoch 127/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2241 - accuracy: 0.9203 - val_loss: 0.1291 - val_accuracy: 0.9560\n",
            "Epoch 128/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1690 - accuracy: 0.9286 - val_loss: 0.1290 - val_accuracy: 0.9451\n",
            "Epoch 129/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.2098 - accuracy: 0.9011 - val_loss: 0.1301 - val_accuracy: 0.9560\n",
            "Epoch 130/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1464 - accuracy: 0.9396 - val_loss: 0.1309 - val_accuracy: 0.9451\n",
            "Epoch 131/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1407 - accuracy: 0.9423 - val_loss: 0.3562 - val_accuracy: 0.9011\n",
            "Epoch 132/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1509 - accuracy: 0.9396 - val_loss: 0.1200 - val_accuracy: 0.9560\n",
            "Epoch 133/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1418 - accuracy: 0.9341 - val_loss: 0.1541 - val_accuracy: 0.9341\n",
            "Epoch 134/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.2005 - accuracy: 0.9231 - val_loss: 0.1566 - val_accuracy: 0.9341\n",
            "Epoch 135/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1413 - accuracy: 0.9258 - val_loss: 0.1227 - val_accuracy: 0.9560\n",
            "Epoch 136/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1709 - accuracy: 0.9341 - val_loss: 0.1248 - val_accuracy: 0.9560\n",
            "Epoch 137/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1761 - accuracy: 0.9121 - val_loss: 0.1250 - val_accuracy: 0.9670\n",
            "Epoch 138/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.9368 - val_loss: 0.1360 - val_accuracy: 0.9341\n",
            "Epoch 139/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1645 - accuracy: 0.9231 - val_loss: 0.1251 - val_accuracy: 0.9560\n",
            "Epoch 140/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1384 - accuracy: 0.9396 - val_loss: 0.1223 - val_accuracy: 0.9560\n",
            "Epoch 141/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1498 - accuracy: 0.9423 - val_loss: 0.1436 - val_accuracy: 0.9341\n",
            "Epoch 142/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1552 - accuracy: 0.9258 - val_loss: 0.1204 - val_accuracy: 0.9670\n",
            "Epoch 143/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1564 - accuracy: 0.9368 - val_loss: 0.1287 - val_accuracy: 0.9451\n",
            "Epoch 144/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1576 - accuracy: 0.9203 - val_loss: 0.1238 - val_accuracy: 0.9560\n",
            "Epoch 145/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1540 - accuracy: 0.9341 - val_loss: 0.1394 - val_accuracy: 0.9341\n",
            "Epoch 146/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1375 - accuracy: 0.9451 - val_loss: 0.1401 - val_accuracy: 0.9560\n",
            "Epoch 147/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1390 - accuracy: 0.9423 - val_loss: 0.1292 - val_accuracy: 0.9451\n",
            "Epoch 148/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1548 - accuracy: 0.9423 - val_loss: 0.3356 - val_accuracy: 0.9011\n",
            "Epoch 149/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9313 - val_loss: 0.1703 - val_accuracy: 0.9231\n",
            "Epoch 150/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1540 - accuracy: 0.9231 - val_loss: 0.1260 - val_accuracy: 0.9451\n",
            "Epoch 151/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1380 - accuracy: 0.9423 - val_loss: 0.1619 - val_accuracy: 0.9560\n",
            "Epoch 152/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1557 - accuracy: 0.9313 - val_loss: 0.1293 - val_accuracy: 0.9560\n",
            "Epoch 153/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1467 - accuracy: 0.9396 - val_loss: 0.1349 - val_accuracy: 0.9451\n",
            "Epoch 154/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1536 - accuracy: 0.9286 - val_loss: 0.1485 - val_accuracy: 0.9451\n",
            "Epoch 155/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1426 - accuracy: 0.9368 - val_loss: 0.2406 - val_accuracy: 0.8901\n",
            "Epoch 156/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1687 - accuracy: 0.9341 - val_loss: 0.1456 - val_accuracy: 0.9560\n",
            "Epoch 157/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1546 - accuracy: 0.9341 - val_loss: 0.1230 - val_accuracy: 0.9670\n",
            "Epoch 158/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1326 - accuracy: 0.9451 - val_loss: 0.1309 - val_accuracy: 0.9670\n",
            "Epoch 159/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1291 - accuracy: 0.9478 - val_loss: 0.1271 - val_accuracy: 0.9670\n",
            "Epoch 160/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1454 - accuracy: 0.9423 - val_loss: 0.1286 - val_accuracy: 0.9670\n",
            "Epoch 161/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1238 - accuracy: 0.9423 - val_loss: 0.1592 - val_accuracy: 0.9560\n",
            "Epoch 162/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1537 - accuracy: 0.9341 - val_loss: 0.1453 - val_accuracy: 0.9451\n",
            "Epoch 163/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1518 - accuracy: 0.9258 - val_loss: 0.1812 - val_accuracy: 0.9451\n",
            "Epoch 164/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1675 - accuracy: 0.9368 - val_loss: 0.2032 - val_accuracy: 0.9011\n",
            "Epoch 165/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1684 - accuracy: 0.9286 - val_loss: 0.1331 - val_accuracy: 0.9341\n",
            "Epoch 166/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1319 - accuracy: 0.9505 - val_loss: 0.1272 - val_accuracy: 0.9670\n",
            "Epoch 167/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1442 - accuracy: 0.9368 - val_loss: 0.1257 - val_accuracy: 0.9670\n",
            "Epoch 168/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1375 - accuracy: 0.9451 - val_loss: 0.2627 - val_accuracy: 0.9011\n",
            "Epoch 169/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1385 - accuracy: 0.9396 - val_loss: 0.1250 - val_accuracy: 0.9670\n",
            "Epoch 170/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1248 - accuracy: 0.9478 - val_loss: 0.2987 - val_accuracy: 0.8901\n",
            "Epoch 171/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2146 - accuracy: 0.9121 - val_loss: 0.1406 - val_accuracy: 0.9560\n",
            "Epoch 172/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1440 - accuracy: 0.9313 - val_loss: 0.1261 - val_accuracy: 0.9451\n",
            "Epoch 173/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1376 - accuracy: 0.9423 - val_loss: 0.1264 - val_accuracy: 0.9670\n",
            "Epoch 174/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1475 - accuracy: 0.9341 - val_loss: 0.1322 - val_accuracy: 0.9670\n",
            "Epoch 175/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1217 - accuracy: 0.9451 - val_loss: 0.1348 - val_accuracy: 0.9341\n",
            "Epoch 176/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1368 - accuracy: 0.9396 - val_loss: 0.1360 - val_accuracy: 0.9670\n",
            "Epoch 177/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1733 - accuracy: 0.9396 - val_loss: 0.1411 - val_accuracy: 0.9560\n",
            "Epoch 178/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1475 - accuracy: 0.9313 - val_loss: 0.1580 - val_accuracy: 0.9231\n",
            "Epoch 179/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1240 - accuracy: 0.9451 - val_loss: 0.1481 - val_accuracy: 0.9451\n",
            "Epoch 180/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1281 - accuracy: 0.9560 - val_loss: 0.1330 - val_accuracy: 0.9341\n",
            "Epoch 181/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1225 - accuracy: 0.9451 - val_loss: 0.1280 - val_accuracy: 0.9670\n",
            "Epoch 182/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1302 - accuracy: 0.9423 - val_loss: 0.1328 - val_accuracy: 0.9451\n",
            "Epoch 183/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1449 - accuracy: 0.9341 - val_loss: 0.1325 - val_accuracy: 0.9451\n",
            "Epoch 184/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1420 - accuracy: 0.9368 - val_loss: 0.1284 - val_accuracy: 0.9670\n",
            "Epoch 185/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1220 - accuracy: 0.9533 - val_loss: 0.1259 - val_accuracy: 0.9451\n",
            "Epoch 186/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1310 - accuracy: 0.9396 - val_loss: 0.1452 - val_accuracy: 0.9451\n",
            "Epoch 187/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1102 - accuracy: 0.9478 - val_loss: 0.1241 - val_accuracy: 0.9560\n",
            "Epoch 188/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1272 - accuracy: 0.9451 - val_loss: 0.1262 - val_accuracy: 0.9560\n",
            "Epoch 189/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1157 - accuracy: 0.9533 - val_loss: 0.2132 - val_accuracy: 0.9341\n",
            "Epoch 190/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1707 - accuracy: 0.9203 - val_loss: 0.1245 - val_accuracy: 0.9560\n",
            "Epoch 191/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1272 - accuracy: 0.9478 - val_loss: 0.1234 - val_accuracy: 0.9670\n",
            "Epoch 192/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1483 - accuracy: 0.9423 - val_loss: 0.2225 - val_accuracy: 0.9341\n",
            "Epoch 193/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1195 - accuracy: 0.9396 - val_loss: 0.1341 - val_accuracy: 0.9670\n",
            "Epoch 194/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.9368 - val_loss: 0.1207 - val_accuracy: 0.9560\n",
            "Epoch 195/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1921 - accuracy: 0.9286 - val_loss: 0.1318 - val_accuracy: 0.9670\n",
            "Epoch 196/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1292 - accuracy: 0.9451 - val_loss: 0.1305 - val_accuracy: 0.9451\n",
            "Epoch 197/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1298 - accuracy: 0.9341 - val_loss: 0.1452 - val_accuracy: 0.9451\n",
            "Epoch 198/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1539 - accuracy: 0.9451 - val_loss: 0.1535 - val_accuracy: 0.9451\n",
            "Epoch 199/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1250 - accuracy: 0.9368 - val_loss: 0.1765 - val_accuracy: 0.9121\n",
            "Epoch 200/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1576 - accuracy: 0.9396 - val_loss: 0.1363 - val_accuracy: 0.9560\n",
            "Epoch 201/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1286 - accuracy: 0.9368 - val_loss: 0.1366 - val_accuracy: 0.9670\n",
            "Epoch 202/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1258 - accuracy: 0.9533 - val_loss: 0.1379 - val_accuracy: 0.9670\n",
            "Epoch 203/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1449 - accuracy: 0.9368 - val_loss: 0.1265 - val_accuracy: 0.9560\n",
            "Epoch 204/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1285 - accuracy: 0.9478 - val_loss: 0.1230 - val_accuracy: 0.9560\n",
            "Epoch 205/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1366 - accuracy: 0.9368 - val_loss: 0.1211 - val_accuracy: 0.9560\n",
            "Epoch 206/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1415 - accuracy: 0.9341 - val_loss: 0.1689 - val_accuracy: 0.9451\n",
            "Epoch 207/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1149 - accuracy: 0.9560 - val_loss: 0.1253 - val_accuracy: 0.9560\n",
            "Epoch 208/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1262 - accuracy: 0.9368 - val_loss: 0.1246 - val_accuracy: 0.9670\n",
            "Epoch 209/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1347 - accuracy: 0.9423 - val_loss: 0.1196 - val_accuracy: 0.9560\n",
            "Epoch 210/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1198 - accuracy: 0.9368 - val_loss: 0.1187 - val_accuracy: 0.9560\n",
            "Epoch 211/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1141 - accuracy: 0.9451 - val_loss: 0.1491 - val_accuracy: 0.9670\n",
            "Epoch 212/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1196 - accuracy: 0.9423 - val_loss: 0.1420 - val_accuracy: 0.9670\n",
            "Epoch 213/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1250 - accuracy: 0.9478 - val_loss: 0.1259 - val_accuracy: 0.9670\n",
            "Epoch 214/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1208 - accuracy: 0.9451 - val_loss: 0.1310 - val_accuracy: 0.9670\n",
            "Epoch 215/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1423 - accuracy: 0.9451 - val_loss: 0.1237 - val_accuracy: 0.9670\n",
            "Epoch 216/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1841 - accuracy: 0.9396 - val_loss: 0.1626 - val_accuracy: 0.9451\n",
            "Epoch 217/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.2257 - accuracy: 0.9203 - val_loss: 0.1471 - val_accuracy: 0.9560\n",
            "Epoch 218/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1833 - accuracy: 0.9451 - val_loss: 0.1511 - val_accuracy: 0.9451\n",
            "Epoch 219/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1377 - accuracy: 0.9258 - val_loss: 0.2596 - val_accuracy: 0.9121\n",
            "Epoch 220/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1553 - accuracy: 0.9451 - val_loss: 0.1443 - val_accuracy: 0.9560\n",
            "Epoch 221/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1452 - accuracy: 0.9313 - val_loss: 0.1639 - val_accuracy: 0.9451\n",
            "Epoch 222/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1408 - accuracy: 0.9423 - val_loss: 0.1354 - val_accuracy: 0.9560\n",
            "Epoch 223/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1148 - accuracy: 0.9423 - val_loss: 0.1503 - val_accuracy: 0.9560\n",
            "Epoch 224/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1444 - accuracy: 0.9313 - val_loss: 0.1368 - val_accuracy: 0.9451\n",
            "Epoch 225/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1265 - accuracy: 0.9423 - val_loss: 0.1672 - val_accuracy: 0.9560\n",
            "Epoch 226/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1756 - accuracy: 0.9176 - val_loss: 0.4203 - val_accuracy: 0.8901\n",
            "Epoch 227/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1208 - accuracy: 0.9423 - val_loss: 0.1710 - val_accuracy: 0.9670\n",
            "Epoch 228/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1498 - accuracy: 0.9368 - val_loss: 0.1362 - val_accuracy: 0.9451\n",
            "Epoch 229/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1350 - accuracy: 0.9451 - val_loss: 0.1507 - val_accuracy: 0.9451\n",
            "Epoch 230/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1142 - accuracy: 0.9560 - val_loss: 0.1246 - val_accuracy: 0.9560\n",
            "Epoch 231/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1307 - accuracy: 0.9368 - val_loss: 0.1390 - val_accuracy: 0.9451\n",
            "Epoch 232/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1146 - accuracy: 0.9533 - val_loss: 0.1493 - val_accuracy: 0.9451\n",
            "Epoch 233/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1524 - accuracy: 0.9368 - val_loss: 0.2209 - val_accuracy: 0.9451\n",
            "Epoch 234/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1483 - accuracy: 0.9423 - val_loss: 0.1631 - val_accuracy: 0.9670\n",
            "Epoch 235/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1584 - accuracy: 0.9396 - val_loss: 0.1396 - val_accuracy: 0.9560\n",
            "Epoch 236/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.9451 - val_loss: 0.1494 - val_accuracy: 0.9451\n",
            "Epoch 237/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1168 - accuracy: 0.9478 - val_loss: 0.1634 - val_accuracy: 0.9560\n",
            "Epoch 238/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.9588 - val_loss: 0.1321 - val_accuracy: 0.9560\n",
            "Epoch 239/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1085 - accuracy: 0.9423 - val_loss: 0.1655 - val_accuracy: 0.9670\n",
            "Epoch 240/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1045 - accuracy: 0.9423 - val_loss: 0.1414 - val_accuracy: 0.9451\n",
            "Epoch 241/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1136 - accuracy: 0.9533 - val_loss: 0.1382 - val_accuracy: 0.9451\n",
            "Epoch 242/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0906 - accuracy: 0.9505 - val_loss: 0.2050 - val_accuracy: 0.9451\n",
            "Epoch 243/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1253 - accuracy: 0.9423 - val_loss: 0.1376 - val_accuracy: 0.9451\n",
            "Epoch 244/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1304 - accuracy: 0.9478 - val_loss: 0.1307 - val_accuracy: 0.9670\n",
            "Epoch 245/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1035 - accuracy: 0.9478 - val_loss: 0.1304 - val_accuracy: 0.9451\n",
            "Epoch 246/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1323 - accuracy: 0.9423 - val_loss: 0.1787 - val_accuracy: 0.9670\n",
            "Epoch 247/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1176 - accuracy: 0.9451 - val_loss: 0.1486 - val_accuracy: 0.9451\n",
            "Epoch 248/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.9588 - val_loss: 0.2257 - val_accuracy: 0.9451\n",
            "Epoch 249/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1204 - accuracy: 0.9396 - val_loss: 0.1231 - val_accuracy: 0.9560\n",
            "Epoch 250/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1036 - accuracy: 0.9533 - val_loss: 0.1551 - val_accuracy: 0.9560\n",
            "Epoch 251/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1440 - accuracy: 0.9451 - val_loss: 0.1848 - val_accuracy: 0.9560\n",
            "Epoch 252/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1165 - accuracy: 0.9451 - val_loss: 0.1266 - val_accuracy: 0.9451\n",
            "Epoch 253/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1047 - accuracy: 0.9478 - val_loss: 0.1684 - val_accuracy: 0.9560\n",
            "Epoch 254/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1004 - accuracy: 0.9560 - val_loss: 0.1301 - val_accuracy: 0.9451\n",
            "Epoch 255/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1036 - accuracy: 0.9698 - val_loss: 0.1265 - val_accuracy: 0.9560\n",
            "Epoch 256/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1312 - accuracy: 0.9423 - val_loss: 0.1379 - val_accuracy: 0.9451\n",
            "Epoch 257/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1164 - accuracy: 0.9478 - val_loss: 0.1217 - val_accuracy: 0.9560\n",
            "Epoch 258/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1168 - accuracy: 0.9560 - val_loss: 0.1336 - val_accuracy: 0.9451\n",
            "Epoch 259/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1143 - accuracy: 0.9451 - val_loss: 0.1311 - val_accuracy: 0.9451\n",
            "Epoch 260/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.9588 - val_loss: 0.1322 - val_accuracy: 0.9670\n",
            "Epoch 261/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1081 - accuracy: 0.9643 - val_loss: 0.2239 - val_accuracy: 0.9231\n",
            "Epoch 262/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1799 - accuracy: 0.9341 - val_loss: 0.1241 - val_accuracy: 0.9670\n",
            "Epoch 263/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1127 - accuracy: 0.9588 - val_loss: 0.1236 - val_accuracy: 0.9560\n",
            "Epoch 264/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1047 - accuracy: 0.9478 - val_loss: 0.1232 - val_accuracy: 0.9560\n",
            "Epoch 265/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1140 - accuracy: 0.9533 - val_loss: 0.2434 - val_accuracy: 0.9341\n",
            "Epoch 266/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1156 - accuracy: 0.9533 - val_loss: 0.1336 - val_accuracy: 0.9670\n",
            "Epoch 267/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0960 - accuracy: 0.9588 - val_loss: 0.1264 - val_accuracy: 0.9451\n",
            "Epoch 268/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0987 - accuracy: 0.9533 - val_loss: 0.1305 - val_accuracy: 0.9670\n",
            "Epoch 269/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1239 - accuracy: 0.9423 - val_loss: 0.1808 - val_accuracy: 0.9670\n",
            "Epoch 270/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1176 - accuracy: 0.9368 - val_loss: 0.1621 - val_accuracy: 0.9670\n",
            "Epoch 271/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1136 - accuracy: 0.9505 - val_loss: 0.1384 - val_accuracy: 0.9451\n",
            "Epoch 272/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1019 - accuracy: 0.9451 - val_loss: 0.1280 - val_accuracy: 0.9451\n",
            "Epoch 273/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1198 - accuracy: 0.9451 - val_loss: 0.1379 - val_accuracy: 0.9451\n",
            "Epoch 274/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1010 - accuracy: 0.9533 - val_loss: 0.1210 - val_accuracy: 0.9560\n",
            "Epoch 275/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0921 - accuracy: 0.9505 - val_loss: 0.1209 - val_accuracy: 0.9560\n",
            "Epoch 276/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1334 - accuracy: 0.9313 - val_loss: 0.1959 - val_accuracy: 0.9560\n",
            "Epoch 277/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1199 - accuracy: 0.9423 - val_loss: 0.1377 - val_accuracy: 0.9560\n",
            "Epoch 278/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1057 - accuracy: 0.9560 - val_loss: 0.1315 - val_accuracy: 0.9451\n",
            "Epoch 279/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1170 - accuracy: 0.9505 - val_loss: 0.1471 - val_accuracy: 0.9670\n",
            "Epoch 280/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1238 - accuracy: 0.9505 - val_loss: 0.1201 - val_accuracy: 0.9560\n",
            "Epoch 281/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1115 - accuracy: 0.9478 - val_loss: 0.1585 - val_accuracy: 0.9451\n",
            "Epoch 282/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1428 - accuracy: 0.9478 - val_loss: 0.1583 - val_accuracy: 0.9670\n",
            "Epoch 283/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.9560 - val_loss: 0.1460 - val_accuracy: 0.9451\n",
            "Epoch 284/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1060 - accuracy: 0.9533 - val_loss: 0.1390 - val_accuracy: 0.9451\n",
            "Epoch 285/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1117 - accuracy: 0.9451 - val_loss: 0.1497 - val_accuracy: 0.9670\n",
            "Epoch 286/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1100 - accuracy: 0.9533 - val_loss: 0.1702 - val_accuracy: 0.9670\n",
            "Epoch 287/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1083 - accuracy: 0.9560 - val_loss: 0.1601 - val_accuracy: 0.9670\n",
            "Epoch 288/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1155 - accuracy: 0.9560 - val_loss: 0.1544 - val_accuracy: 0.9451\n",
            "Epoch 289/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1237 - accuracy: 0.9478 - val_loss: 0.1503 - val_accuracy: 0.9560\n",
            "Epoch 290/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0952 - accuracy: 0.9615 - val_loss: 0.1474 - val_accuracy: 0.9560\n",
            "Epoch 291/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1252 - accuracy: 0.9505 - val_loss: 0.1551 - val_accuracy: 0.9560\n",
            "Epoch 292/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1407 - accuracy: 0.9396 - val_loss: 0.1229 - val_accuracy: 0.9560\n",
            "Epoch 293/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0903 - accuracy: 0.9588 - val_loss: 0.1172 - val_accuracy: 0.9670\n",
            "Epoch 294/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1187 - accuracy: 0.9505 - val_loss: 0.1401 - val_accuracy: 0.9670\n",
            "Epoch 295/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1177 - accuracy: 0.9643 - val_loss: 0.1885 - val_accuracy: 0.9670\n",
            "Epoch 296/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1252 - accuracy: 0.9423 - val_loss: 0.1274 - val_accuracy: 0.9451\n",
            "Epoch 297/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.9533 - val_loss: 0.1883 - val_accuracy: 0.9560\n",
            "Epoch 298/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1046 - accuracy: 0.9533 - val_loss: 0.1327 - val_accuracy: 0.9670\n",
            "Epoch 299/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0890 - accuracy: 0.9560 - val_loss: 0.1415 - val_accuracy: 0.9670\n",
            "Epoch 300/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1143 - accuracy: 0.9505 - val_loss: 0.1725 - val_accuracy: 0.9670\n",
            "Epoch 301/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.9588 - val_loss: 0.1557 - val_accuracy: 0.9670\n",
            "Epoch 302/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 0.9368 - val_loss: 0.1905 - val_accuracy: 0.9560\n",
            "Epoch 303/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1028 - accuracy: 0.9533 - val_loss: 0.1333 - val_accuracy: 0.9670\n",
            "Epoch 304/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0872 - accuracy: 0.9698 - val_loss: 0.1440 - val_accuracy: 0.9670\n",
            "Epoch 305/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0970 - accuracy: 0.9560 - val_loss: 0.1372 - val_accuracy: 0.9670\n",
            "Epoch 306/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.9615 - val_loss: 0.1694 - val_accuracy: 0.9670\n",
            "Epoch 307/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0977 - accuracy: 0.9560 - val_loss: 0.1187 - val_accuracy: 0.9670\n",
            "Epoch 308/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1088 - accuracy: 0.9505 - val_loss: 0.1252 - val_accuracy: 0.9451\n",
            "Epoch 309/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0970 - accuracy: 0.9533 - val_loss: 0.1456 - val_accuracy: 0.9670\n",
            "Epoch 310/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0882 - accuracy: 0.9533 - val_loss: 0.2776 - val_accuracy: 0.9231\n",
            "Epoch 311/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0979 - accuracy: 0.9588 - val_loss: 0.1348 - val_accuracy: 0.9560\n",
            "Epoch 312/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.9478 - val_loss: 0.1281 - val_accuracy: 0.9670\n",
            "Epoch 313/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1019 - accuracy: 0.9533 - val_loss: 0.1504 - val_accuracy: 0.9670\n",
            "Epoch 314/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1002 - accuracy: 0.9588 - val_loss: 0.1522 - val_accuracy: 0.9670\n",
            "Epoch 315/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1072 - accuracy: 0.9505 - val_loss: 0.1186 - val_accuracy: 0.9560\n",
            "Epoch 316/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1017 - accuracy: 0.9478 - val_loss: 0.1241 - val_accuracy: 0.9670\n",
            "Epoch 317/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1079 - accuracy: 0.9478 - val_loss: 0.1174 - val_accuracy: 0.9560\n",
            "Epoch 318/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1265 - accuracy: 0.9505 - val_loss: 0.1676 - val_accuracy: 0.9670\n",
            "Epoch 319/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0999 - accuracy: 0.9423 - val_loss: 0.1241 - val_accuracy: 0.9560\n",
            "Epoch 320/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1072 - accuracy: 0.9451 - val_loss: 0.1257 - val_accuracy: 0.9670\n",
            "Epoch 321/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0910 - accuracy: 0.9560 - val_loss: 0.1649 - val_accuracy: 0.9670\n",
            "Epoch 322/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.9588 - val_loss: 0.1236 - val_accuracy: 0.9670\n",
            "Epoch 323/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9615 - val_loss: 0.1396 - val_accuracy: 0.9451\n",
            "Epoch 324/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.9615 - val_loss: 0.1426 - val_accuracy: 0.9670\n",
            "Epoch 325/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0887 - accuracy: 0.9478 - val_loss: 0.1560 - val_accuracy: 0.9670\n",
            "Epoch 326/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0860 - accuracy: 0.9588 - val_loss: 0.1393 - val_accuracy: 0.9670\n",
            "Epoch 327/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0880 - accuracy: 0.9615 - val_loss: 0.1258 - val_accuracy: 0.9670\n",
            "Epoch 328/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1124 - accuracy: 0.9478 - val_loss: 0.1921 - val_accuracy: 0.9670\n",
            "Epoch 329/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1184 - accuracy: 0.9341 - val_loss: 0.1361 - val_accuracy: 0.9670\n",
            "Epoch 330/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0828 - accuracy: 0.9560 - val_loss: 0.1802 - val_accuracy: 0.9670\n",
            "Epoch 331/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1376 - accuracy: 0.9478 - val_loss: 0.1170 - val_accuracy: 0.9560\n",
            "Epoch 332/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0979 - accuracy: 0.9505 - val_loss: 0.1321 - val_accuracy: 0.9451\n",
            "Epoch 333/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0901 - accuracy: 0.9560 - val_loss: 0.1402 - val_accuracy: 0.9670\n",
            "Epoch 334/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1039 - accuracy: 0.9478 - val_loss: 0.1673 - val_accuracy: 0.9670\n",
            "Epoch 335/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1008 - accuracy: 0.9588 - val_loss: 0.1222 - val_accuracy: 0.9670\n",
            "Epoch 336/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.9533 - val_loss: 0.1323 - val_accuracy: 0.9670\n",
            "Epoch 337/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0993 - accuracy: 0.9533 - val_loss: 0.1232 - val_accuracy: 0.9670\n",
            "Epoch 338/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1029 - accuracy: 0.9423 - val_loss: 0.1339 - val_accuracy: 0.9670\n",
            "Epoch 339/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0860 - accuracy: 0.9670 - val_loss: 0.1233 - val_accuracy: 0.9670\n",
            "Epoch 340/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0952 - accuracy: 0.9478 - val_loss: 0.1204 - val_accuracy: 0.9670\n",
            "Epoch 341/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0944 - accuracy: 0.9615 - val_loss: 0.1360 - val_accuracy: 0.9560\n",
            "Epoch 342/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.9588 - val_loss: 0.1433 - val_accuracy: 0.9670\n",
            "Epoch 343/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 0.9698 - val_loss: 0.1354 - val_accuracy: 0.9670\n",
            "Epoch 344/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.9588 - val_loss: 0.1316 - val_accuracy: 0.9560\n",
            "Epoch 345/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0959 - accuracy: 0.9560 - val_loss: 0.1264 - val_accuracy: 0.9451\n",
            "Epoch 346/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0947 - accuracy: 0.9643 - val_loss: 0.1333 - val_accuracy: 0.9670\n",
            "Epoch 347/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0918 - accuracy: 0.9478 - val_loss: 0.1712 - val_accuracy: 0.9670\n",
            "Epoch 348/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0876 - accuracy: 0.9560 - val_loss: 0.1203 - val_accuracy: 0.9670\n",
            "Epoch 349/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.9588 - val_loss: 0.1378 - val_accuracy: 0.9670\n",
            "Epoch 350/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1028 - accuracy: 0.9560 - val_loss: 0.1385 - val_accuracy: 0.9670\n",
            "Epoch 351/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.9588 - val_loss: 0.1754 - val_accuracy: 0.9670\n",
            "Epoch 352/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0846 - accuracy: 0.9698 - val_loss: 0.1402 - val_accuracy: 0.9560\n",
            "Epoch 353/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1068 - accuracy: 0.9478 - val_loss: 0.1725 - val_accuracy: 0.9670\n",
            "Epoch 354/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.9533 - val_loss: 0.1703 - val_accuracy: 0.9670\n",
            "Epoch 355/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0924 - accuracy: 0.9643 - val_loss: 0.1512 - val_accuracy: 0.9560\n",
            "Epoch 356/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1035 - accuracy: 0.9643 - val_loss: 0.1709 - val_accuracy: 0.9670\n",
            "Epoch 357/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1080 - accuracy: 0.9560 - val_loss: 0.1269 - val_accuracy: 0.9670\n",
            "Epoch 358/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1128 - accuracy: 0.9478 - val_loss: 0.1387 - val_accuracy: 0.9670\n",
            "Epoch 359/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0857 - accuracy: 0.9588 - val_loss: 0.1235 - val_accuracy: 0.9670\n",
            "Epoch 360/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1078 - accuracy: 0.9533 - val_loss: 0.1275 - val_accuracy: 0.9451\n",
            "Epoch 361/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0784 - accuracy: 0.9560 - val_loss: 0.2313 - val_accuracy: 0.9560\n",
            "Epoch 362/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.9560 - val_loss: 0.1970 - val_accuracy: 0.9560\n",
            "Epoch 363/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1238 - accuracy: 0.9478 - val_loss: 0.1225 - val_accuracy: 0.9670\n",
            "Epoch 364/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0906 - accuracy: 0.9670 - val_loss: 0.1355 - val_accuracy: 0.9670\n",
            "Epoch 365/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.9615 - val_loss: 0.1802 - val_accuracy: 0.9670\n",
            "Epoch 366/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.9560 - val_loss: 0.1362 - val_accuracy: 0.9560\n",
            "Epoch 367/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.9670 - val_loss: 0.1569 - val_accuracy: 0.9670\n",
            "Epoch 368/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0872 - accuracy: 0.9505 - val_loss: 0.1410 - val_accuracy: 0.9560\n",
            "Epoch 369/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.9533 - val_loss: 0.1290 - val_accuracy: 0.9670\n",
            "Epoch 370/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0824 - accuracy: 0.9698 - val_loss: 0.1643 - val_accuracy: 0.9670\n",
            "Epoch 371/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0926 - accuracy: 0.9698 - val_loss: 0.1915 - val_accuracy: 0.9670\n",
            "Epoch 372/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1022 - accuracy: 0.9478 - val_loss: 0.1317 - val_accuracy: 0.9451\n",
            "Epoch 373/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.9560 - val_loss: 0.1345 - val_accuracy: 0.9670\n",
            "Epoch 374/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.9643 - val_loss: 0.1287 - val_accuracy: 0.9670\n",
            "Epoch 375/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9533 - val_loss: 0.1401 - val_accuracy: 0.9670\n",
            "Epoch 376/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0781 - accuracy: 0.9643 - val_loss: 0.1500 - val_accuracy: 0.9560\n",
            "Epoch 377/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0996 - accuracy: 0.9560 - val_loss: 0.1256 - val_accuracy: 0.9670\n",
            "Epoch 378/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0860 - accuracy: 0.9643 - val_loss: 0.1570 - val_accuracy: 0.9670\n",
            "Epoch 379/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0843 - accuracy: 0.9698 - val_loss: 0.1323 - val_accuracy: 0.9780\n",
            "Epoch 380/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9505 - val_loss: 0.1592 - val_accuracy: 0.9670\n",
            "Epoch 381/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0873 - accuracy: 0.9505 - val_loss: 0.1301 - val_accuracy: 0.9670\n",
            "Epoch 382/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1020 - accuracy: 0.9560 - val_loss: 0.2030 - val_accuracy: 0.9560\n",
            "Epoch 383/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0969 - accuracy: 0.9588 - val_loss: 0.1306 - val_accuracy: 0.9670\n",
            "Epoch 384/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0890 - accuracy: 0.9588 - val_loss: 0.1409 - val_accuracy: 0.9670\n",
            "Epoch 385/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0934 - accuracy: 0.9588 - val_loss: 0.1262 - val_accuracy: 0.9670\n",
            "Epoch 386/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1026 - accuracy: 0.9505 - val_loss: 0.1290 - val_accuracy: 0.9670\n",
            "Epoch 387/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0819 - accuracy: 0.9615 - val_loss: 0.1565 - val_accuracy: 0.9670\n",
            "Epoch 388/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0996 - accuracy: 0.9396 - val_loss: 0.1317 - val_accuracy: 0.9560\n",
            "Epoch 389/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.9615 - val_loss: 0.1584 - val_accuracy: 0.9560\n",
            "Epoch 390/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1063 - accuracy: 0.9560 - val_loss: 0.1262 - val_accuracy: 0.9670\n",
            "Epoch 391/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.9588 - val_loss: 0.1399 - val_accuracy: 0.9670\n",
            "Epoch 392/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1025 - accuracy: 0.9560 - val_loss: 0.1281 - val_accuracy: 0.9670\n",
            "Epoch 393/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1034 - accuracy: 0.9698 - val_loss: 0.1508 - val_accuracy: 0.9670\n",
            "Epoch 394/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.9615 - val_loss: 0.1267 - val_accuracy: 0.9670\n",
            "Epoch 395/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9780 - val_loss: 0.1395 - val_accuracy: 0.9560\n",
            "Epoch 396/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0842 - accuracy: 0.9560 - val_loss: 0.1378 - val_accuracy: 0.9670\n",
            "Epoch 397/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9588 - val_loss: 0.1317 - val_accuracy: 0.9670\n",
            "Epoch 398/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0903 - accuracy: 0.9615 - val_loss: 0.1864 - val_accuracy: 0.9560\n",
            "Epoch 399/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9560 - val_loss: 0.1644 - val_accuracy: 0.9670\n",
            "Epoch 400/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.9670 - val_loss: 0.1474 - val_accuracy: 0.9670\n",
            "Epoch 401/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0723 - accuracy: 0.9698 - val_loss: 0.1430 - val_accuracy: 0.9670\n",
            "Epoch 402/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0916 - accuracy: 0.9615 - val_loss: 0.2058 - val_accuracy: 0.9560\n",
            "Epoch 403/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1095 - accuracy: 0.9451 - val_loss: 0.1425 - val_accuracy: 0.9560\n",
            "Epoch 404/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1120 - accuracy: 0.9478 - val_loss: 0.1241 - val_accuracy: 0.9780\n",
            "Epoch 405/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0790 - accuracy: 0.9505 - val_loss: 0.1455 - val_accuracy: 0.9560\n",
            "Epoch 406/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0906 - accuracy: 0.9560 - val_loss: 0.1268 - val_accuracy: 0.9670\n",
            "Epoch 407/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.9505 - val_loss: 0.1447 - val_accuracy: 0.9670\n",
            "Epoch 408/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0906 - accuracy: 0.9670 - val_loss: 0.1467 - val_accuracy: 0.9670\n",
            "Epoch 409/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0768 - accuracy: 0.9670 - val_loss: 0.1357 - val_accuracy: 0.9670\n",
            "Epoch 410/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.9588 - val_loss: 0.1331 - val_accuracy: 0.9670\n",
            "Epoch 411/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1391 - accuracy: 0.9451 - val_loss: 0.2100 - val_accuracy: 0.9121\n",
            "Epoch 412/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1294 - accuracy: 0.9451 - val_loss: 0.1995 - val_accuracy: 0.9560\n",
            "Epoch 413/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1294 - accuracy: 0.9615 - val_loss: 0.2018 - val_accuracy: 0.9560\n",
            "Epoch 414/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.9533 - val_loss: 0.1229 - val_accuracy: 0.9670\n",
            "Epoch 415/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1017 - accuracy: 0.9670 - val_loss: 0.1247 - val_accuracy: 0.9670\n",
            "Epoch 416/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0893 - accuracy: 0.9615 - val_loss: 0.1375 - val_accuracy: 0.9670\n",
            "Epoch 417/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0952 - accuracy: 0.9505 - val_loss: 0.1305 - val_accuracy: 0.9560\n",
            "Epoch 418/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 0.9588 - val_loss: 0.1851 - val_accuracy: 0.9670\n",
            "Epoch 419/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0950 - accuracy: 0.9588 - val_loss: 0.1850 - val_accuracy: 0.9560\n",
            "Epoch 420/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1013 - accuracy: 0.9643 - val_loss: 0.1242 - val_accuracy: 0.9780\n",
            "Epoch 421/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1303 - accuracy: 0.9533 - val_loss: 0.1842 - val_accuracy: 0.9670\n",
            "Epoch 422/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0982 - accuracy: 0.9670 - val_loss: 0.1468 - val_accuracy: 0.9670\n",
            "Epoch 423/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.9698 - val_loss: 0.1405 - val_accuracy: 0.9670\n",
            "Epoch 424/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.9643 - val_loss: 0.1352 - val_accuracy: 0.9560\n",
            "Epoch 425/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0685 - accuracy: 0.9698 - val_loss: 0.2820 - val_accuracy: 0.9560\n",
            "Epoch 426/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1353 - accuracy: 0.9396 - val_loss: 0.1278 - val_accuracy: 0.9670\n",
            "Epoch 427/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.9560 - val_loss: 0.2009 - val_accuracy: 0.9670\n",
            "Epoch 428/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.9533 - val_loss: 0.1605 - val_accuracy: 0.9670\n",
            "Epoch 429/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0909 - accuracy: 0.9505 - val_loss: 0.1296 - val_accuracy: 0.9560\n",
            "Epoch 430/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.9670 - val_loss: 0.1576 - val_accuracy: 0.9670\n",
            "Epoch 431/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0897 - accuracy: 0.9505 - val_loss: 0.2155 - val_accuracy: 0.9560\n",
            "Epoch 432/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9670 - val_loss: 0.1470 - val_accuracy: 0.9451\n",
            "Epoch 433/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1220 - accuracy: 0.9533 - val_loss: 0.1679 - val_accuracy: 0.9670\n",
            "Epoch 434/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.9533 - val_loss: 0.1398 - val_accuracy: 0.9670\n",
            "Epoch 435/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1005 - accuracy: 0.9615 - val_loss: 0.2030 - val_accuracy: 0.9560\n",
            "Epoch 436/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9670 - val_loss: 0.1397 - val_accuracy: 0.9560\n",
            "Epoch 437/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0667 - accuracy: 0.9670 - val_loss: 0.1849 - val_accuracy: 0.9670\n",
            "Epoch 438/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0831 - accuracy: 0.9588 - val_loss: 0.1503 - val_accuracy: 0.9670\n",
            "Epoch 439/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0831 - accuracy: 0.9588 - val_loss: 0.1429 - val_accuracy: 0.9560\n",
            "Epoch 440/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.9698 - val_loss: 0.1265 - val_accuracy: 0.9670\n",
            "Epoch 441/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.9560 - val_loss: 0.1879 - val_accuracy: 0.9341\n",
            "Epoch 442/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.9670 - val_loss: 0.2329 - val_accuracy: 0.9560\n",
            "Epoch 443/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0778 - accuracy: 0.9643 - val_loss: 0.1459 - val_accuracy: 0.9670\n",
            "Epoch 444/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0959 - accuracy: 0.9588 - val_loss: 0.1268 - val_accuracy: 0.9670\n",
            "Epoch 445/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9670 - val_loss: 0.1753 - val_accuracy: 0.9670\n",
            "Epoch 446/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1253 - accuracy: 0.9396 - val_loss: 0.1364 - val_accuracy: 0.9670\n",
            "Epoch 447/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0970 - accuracy: 0.9505 - val_loss: 0.1705 - val_accuracy: 0.9231\n",
            "Epoch 448/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.9698 - val_loss: 0.1472 - val_accuracy: 0.9670\n",
            "Epoch 449/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.9588 - val_loss: 0.1266 - val_accuracy: 0.9780\n",
            "Epoch 450/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0924 - accuracy: 0.9643 - val_loss: 0.2152 - val_accuracy: 0.9560\n",
            "Epoch 451/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.9533 - val_loss: 0.1894 - val_accuracy: 0.9670\n",
            "Epoch 452/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0756 - accuracy: 0.9615 - val_loss: 0.1665 - val_accuracy: 0.9670\n",
            "Epoch 453/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0692 - accuracy: 0.9588 - val_loss: 0.1475 - val_accuracy: 0.9670\n",
            "Epoch 454/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9725 - val_loss: 0.1522 - val_accuracy: 0.9670\n",
            "Epoch 455/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1108 - accuracy: 0.9533 - val_loss: 0.1457 - val_accuracy: 0.9670\n",
            "Epoch 456/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9670 - val_loss: 0.1456 - val_accuracy: 0.9560\n",
            "Epoch 457/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0926 - accuracy: 0.9505 - val_loss: 0.1291 - val_accuracy: 0.9560\n",
            "Epoch 458/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.9670 - val_loss: 0.1503 - val_accuracy: 0.9670\n",
            "Epoch 459/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.9643 - val_loss: 0.1312 - val_accuracy: 0.9670\n",
            "Epoch 460/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0920 - accuracy: 0.9698 - val_loss: 0.2712 - val_accuracy: 0.9560\n",
            "Epoch 461/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0928 - accuracy: 0.9643 - val_loss: 0.1557 - val_accuracy: 0.9670\n",
            "Epoch 462/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0881 - accuracy: 0.9560 - val_loss: 0.1305 - val_accuracy: 0.9670\n",
            "Epoch 463/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0759 - accuracy: 0.9643 - val_loss: 0.1585 - val_accuracy: 0.9670\n",
            "Epoch 464/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.9643 - val_loss: 0.1351 - val_accuracy: 0.9670\n",
            "Epoch 465/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0898 - accuracy: 0.9615 - val_loss: 0.1882 - val_accuracy: 0.9670\n",
            "Epoch 466/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.9588 - val_loss: 0.1336 - val_accuracy: 0.9670\n",
            "Epoch 467/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9533 - val_loss: 0.1439 - val_accuracy: 0.9670\n",
            "Epoch 468/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9670 - val_loss: 0.1288 - val_accuracy: 0.9780\n",
            "Epoch 469/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0796 - accuracy: 0.9643 - val_loss: 0.1586 - val_accuracy: 0.9560\n",
            "Epoch 470/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0852 - accuracy: 0.9670 - val_loss: 0.1315 - val_accuracy: 0.9780\n",
            "Epoch 471/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0830 - accuracy: 0.9643 - val_loss: 0.1316 - val_accuracy: 0.9670\n",
            "Epoch 472/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9643 - val_loss: 0.1796 - val_accuracy: 0.9451\n",
            "Epoch 473/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0907 - accuracy: 0.9615 - val_loss: 0.2245 - val_accuracy: 0.9560\n",
            "Epoch 474/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9643 - val_loss: 0.1527 - val_accuracy: 0.9670\n",
            "Epoch 475/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.9643 - val_loss: 0.1548 - val_accuracy: 0.9670\n",
            "Epoch 476/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1005 - accuracy: 0.9588 - val_loss: 0.2730 - val_accuracy: 0.9560\n",
            "Epoch 477/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0770 - accuracy: 0.9670 - val_loss: 0.1414 - val_accuracy: 0.9670\n",
            "Epoch 478/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0730 - accuracy: 0.9725 - val_loss: 0.2009 - val_accuracy: 0.9560\n",
            "Epoch 479/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.9451 - val_loss: 0.1411 - val_accuracy: 0.9670\n",
            "Epoch 480/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0697 - accuracy: 0.9698 - val_loss: 0.1911 - val_accuracy: 0.9670\n",
            "Epoch 481/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9615 - val_loss: 0.2224 - val_accuracy: 0.9670\n",
            "Epoch 482/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0703 - accuracy: 0.9643 - val_loss: 0.1361 - val_accuracy: 0.9780\n",
            "Epoch 483/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.9588 - val_loss: 0.1409 - val_accuracy: 0.9670\n",
            "Epoch 484/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0765 - accuracy: 0.9670 - val_loss: 0.1401 - val_accuracy: 0.9670\n",
            "Epoch 485/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0891 - accuracy: 0.9588 - val_loss: 0.1428 - val_accuracy: 0.9670\n",
            "Epoch 486/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9725 - val_loss: 0.1434 - val_accuracy: 0.9670\n",
            "Epoch 487/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.9698 - val_loss: 0.1723 - val_accuracy: 0.9670\n",
            "Epoch 488/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9560 - val_loss: 0.1508 - val_accuracy: 0.9670\n",
            "Epoch 489/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0935 - accuracy: 0.9560 - val_loss: 0.1529 - val_accuracy: 0.9670\n",
            "Epoch 490/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9643 - val_loss: 0.1611 - val_accuracy: 0.9670\n",
            "Epoch 491/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0599 - accuracy: 0.9643 - val_loss: 0.1640 - val_accuracy: 0.9670\n",
            "Epoch 492/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0551 - accuracy: 0.9780 - val_loss: 0.2306 - val_accuracy: 0.9670\n",
            "Epoch 493/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1425 - accuracy: 0.9286 - val_loss: 0.1610 - val_accuracy: 0.9670\n",
            "Epoch 494/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0866 - accuracy: 0.9615 - val_loss: 0.1586 - val_accuracy: 0.9560\n",
            "Epoch 495/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.9560 - val_loss: 0.1489 - val_accuracy: 0.9670\n",
            "Epoch 496/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9670 - val_loss: 0.1357 - val_accuracy: 0.9670\n",
            "Epoch 497/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.9588 - val_loss: 0.1409 - val_accuracy: 0.9780\n",
            "Epoch 498/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1049 - accuracy: 0.9588 - val_loss: 0.1924 - val_accuracy: 0.9670\n",
            "Epoch 499/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1021 - accuracy: 0.9588 - val_loss: 0.1409 - val_accuracy: 0.9670\n",
            "Epoch 500/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9643 - val_loss: 0.1608 - val_accuracy: 0.9670\n",
            "Epoch 501/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1028 - accuracy: 0.9533 - val_loss: 0.2159 - val_accuracy: 0.9670\n",
            "Epoch 502/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0950 - accuracy: 0.9588 - val_loss: 0.1645 - val_accuracy: 0.9670\n",
            "Epoch 503/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0868 - accuracy: 0.9560 - val_loss: 0.1507 - val_accuracy: 0.9560\n",
            "Epoch 504/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0822 - accuracy: 0.9643 - val_loss: 0.1370 - val_accuracy: 0.9780\n",
            "Epoch 505/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.9698 - val_loss: 0.2600 - val_accuracy: 0.9670\n",
            "Epoch 506/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1015 - accuracy: 0.9451 - val_loss: 0.1476 - val_accuracy: 0.9560\n",
            "Epoch 507/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.9615 - val_loss: 0.1676 - val_accuracy: 0.9451\n",
            "Epoch 508/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0723 - accuracy: 0.9615 - val_loss: 0.1663 - val_accuracy: 0.9670\n",
            "Epoch 509/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0861 - accuracy: 0.9588 - val_loss: 0.1935 - val_accuracy: 0.9670\n",
            "Epoch 510/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.9780 - val_loss: 0.1554 - val_accuracy: 0.9670\n",
            "Epoch 511/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0839 - accuracy: 0.9615 - val_loss: 0.2274 - val_accuracy: 0.9560\n",
            "Epoch 512/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9615 - val_loss: 0.1466 - val_accuracy: 0.9670\n",
            "Epoch 513/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0668 - accuracy: 0.9615 - val_loss: 0.1489 - val_accuracy: 0.9780\n",
            "Epoch 514/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0823 - accuracy: 0.9588 - val_loss: 0.1510 - val_accuracy: 0.9670\n",
            "Epoch 515/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.9588 - val_loss: 0.1901 - val_accuracy: 0.9670\n",
            "Epoch 516/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9560 - val_loss: 0.1542 - val_accuracy: 0.9670\n",
            "Epoch 517/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0760 - accuracy: 0.9533 - val_loss: 0.2269 - val_accuracy: 0.9670\n",
            "Epoch 518/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.9670 - val_loss: 0.1520 - val_accuracy: 0.9560\n",
            "Epoch 519/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1026 - accuracy: 0.9643 - val_loss: 0.1403 - val_accuracy: 0.9560\n",
            "Epoch 520/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.9670 - val_loss: 0.2005 - val_accuracy: 0.9670\n",
            "Epoch 521/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 0.9615 - val_loss: 0.1398 - val_accuracy: 0.9670\n",
            "Epoch 522/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.9560 - val_loss: 0.1557 - val_accuracy: 0.9560\n",
            "Epoch 523/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0801 - accuracy: 0.9560 - val_loss: 0.2241 - val_accuracy: 0.9670\n",
            "Epoch 524/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1007 - accuracy: 0.9588 - val_loss: 0.1455 - val_accuracy: 0.9780\n",
            "Epoch 525/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9615 - val_loss: 0.1508 - val_accuracy: 0.9670\n",
            "Epoch 526/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.9698 - val_loss: 0.1382 - val_accuracy: 0.9780\n",
            "Epoch 527/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.9670 - val_loss: 0.2135 - val_accuracy: 0.9670\n",
            "Epoch 528/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.9505 - val_loss: 0.1514 - val_accuracy: 0.9560\n",
            "Epoch 529/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0850 - accuracy: 0.9643 - val_loss: 0.1490 - val_accuracy: 0.9780\n",
            "Epoch 530/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.9505 - val_loss: 0.1498 - val_accuracy: 0.9670\n",
            "Epoch 531/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.9588 - val_loss: 0.2081 - val_accuracy: 0.9231\n",
            "Epoch 532/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0758 - accuracy: 0.9698 - val_loss: 0.1411 - val_accuracy: 0.9670\n",
            "Epoch 533/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.9725 - val_loss: 0.1664 - val_accuracy: 0.9670\n",
            "Epoch 534/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0762 - accuracy: 0.9615 - val_loss: 0.2646 - val_accuracy: 0.9560\n",
            "Epoch 535/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0889 - accuracy: 0.9643 - val_loss: 0.1879 - val_accuracy: 0.9670\n",
            "Epoch 536/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9560 - val_loss: 0.1385 - val_accuracy: 0.9670\n",
            "Epoch 537/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0942 - accuracy: 0.9533 - val_loss: 0.1496 - val_accuracy: 0.9780\n",
            "Epoch 538/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.9533 - val_loss: 0.1887 - val_accuracy: 0.9670\n",
            "Epoch 539/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.9698 - val_loss: 0.1409 - val_accuracy: 0.9670\n",
            "Epoch 540/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.9588 - val_loss: 0.1678 - val_accuracy: 0.9670\n",
            "Epoch 541/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9670 - val_loss: 0.2321 - val_accuracy: 0.9560\n",
            "Epoch 542/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1021 - accuracy: 0.9615 - val_loss: 0.1772 - val_accuracy: 0.9670\n",
            "Epoch 543/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9615 - val_loss: 0.1548 - val_accuracy: 0.9451\n",
            "Epoch 544/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0792 - accuracy: 0.9615 - val_loss: 0.2310 - val_accuracy: 0.9670\n",
            "Epoch 545/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1055 - accuracy: 0.9451 - val_loss: 0.1575 - val_accuracy: 0.9670\n",
            "Epoch 546/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0819 - accuracy: 0.9533 - val_loss: 0.1598 - val_accuracy: 0.9670\n",
            "Epoch 547/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.9451 - val_loss: 0.1387 - val_accuracy: 0.9560\n",
            "Epoch 548/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 0.9670 - val_loss: 0.2209 - val_accuracy: 0.9670\n",
            "Epoch 549/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0658 - accuracy: 0.9615 - val_loss: 0.2294 - val_accuracy: 0.9670\n",
            "Epoch 550/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9725 - val_loss: 0.1413 - val_accuracy: 0.9670\n",
            "Epoch 551/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9753 - val_loss: 0.1557 - val_accuracy: 0.9670\n",
            "Epoch 552/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0937 - accuracy: 0.9615 - val_loss: 0.1733 - val_accuracy: 0.9670\n",
            "Epoch 553/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.9533 - val_loss: 0.2211 - val_accuracy: 0.9560\n",
            "Epoch 554/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.9615 - val_loss: 0.2047 - val_accuracy: 0.9670\n",
            "Epoch 555/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0806 - accuracy: 0.9643 - val_loss: 0.1615 - val_accuracy: 0.9451\n",
            "Epoch 556/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0729 - accuracy: 0.9643 - val_loss: 0.2066 - val_accuracy: 0.9670\n",
            "Epoch 557/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9643 - val_loss: 0.1503 - val_accuracy: 0.9560\n",
            "Epoch 558/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0863 - accuracy: 0.9615 - val_loss: 0.1914 - val_accuracy: 0.9670\n",
            "Epoch 559/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9698 - val_loss: 0.1678 - val_accuracy: 0.9670\n",
            "Epoch 560/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9643 - val_loss: 0.1507 - val_accuracy: 0.9780\n",
            "Epoch 561/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0850 - accuracy: 0.9643 - val_loss: 0.1557 - val_accuracy: 0.9780\n",
            "Epoch 562/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0628 - accuracy: 0.9698 - val_loss: 0.2430 - val_accuracy: 0.9670\n",
            "Epoch 563/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9698 - val_loss: 0.1644 - val_accuracy: 0.9780\n",
            "Epoch 564/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.9670 - val_loss: 0.2267 - val_accuracy: 0.9670\n",
            "Epoch 565/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1369 - accuracy: 0.9396 - val_loss: 0.1532 - val_accuracy: 0.9670\n",
            "Epoch 566/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.9670 - val_loss: 0.1831 - val_accuracy: 0.9670\n",
            "Epoch 567/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.9698 - val_loss: 0.2804 - val_accuracy: 0.9011\n",
            "Epoch 568/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1185 - accuracy: 0.9368 - val_loss: 0.1591 - val_accuracy: 0.9670\n",
            "Epoch 569/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.9643 - val_loss: 0.1524 - val_accuracy: 0.9670\n",
            "Epoch 570/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0786 - accuracy: 0.9615 - val_loss: 0.1556 - val_accuracy: 0.9560\n",
            "Epoch 571/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.9615 - val_loss: 0.1550 - val_accuracy: 0.9780\n",
            "Epoch 572/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0646 - accuracy: 0.9670 - val_loss: 0.1496 - val_accuracy: 0.9670\n",
            "Epoch 573/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0795 - accuracy: 0.9698 - val_loss: 0.1600 - val_accuracy: 0.9670\n",
            "Epoch 574/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0713 - accuracy: 0.9643 - val_loss: 0.1573 - val_accuracy: 0.9780\n",
            "Epoch 575/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.1172 - accuracy: 0.9533 - val_loss: 0.1590 - val_accuracy: 0.9670\n",
            "Epoch 576/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0856 - accuracy: 0.9560 - val_loss: 0.2305 - val_accuracy: 0.9670\n",
            "Epoch 577/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.9753 - val_loss: 0.1489 - val_accuracy: 0.9780\n",
            "Epoch 578/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0675 - accuracy: 0.9698 - val_loss: 0.1503 - val_accuracy: 0.9780\n",
            "Epoch 579/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0906 - accuracy: 0.9560 - val_loss: 0.1634 - val_accuracy: 0.9670\n",
            "Epoch 580/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9698 - val_loss: 0.1967 - val_accuracy: 0.9341\n",
            "Epoch 581/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1116 - accuracy: 0.9423 - val_loss: 0.1522 - val_accuracy: 0.9670\n",
            "Epoch 582/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0855 - accuracy: 0.9670 - val_loss: 0.1513 - val_accuracy: 0.9560\n",
            "Epoch 583/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0639 - accuracy: 0.9670 - val_loss: 0.1800 - val_accuracy: 0.9670\n",
            "Epoch 584/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9533 - val_loss: 0.1817 - val_accuracy: 0.9670\n",
            "Epoch 585/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9615 - val_loss: 0.1749 - val_accuracy: 0.9670\n",
            "Epoch 586/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0909 - accuracy: 0.9643 - val_loss: 0.1747 - val_accuracy: 0.9670\n",
            "Epoch 587/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.9698 - val_loss: 0.1858 - val_accuracy: 0.9670\n",
            "Epoch 588/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.9643 - val_loss: 0.2134 - val_accuracy: 0.9670\n",
            "Epoch 589/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0586 - accuracy: 0.9698 - val_loss: 0.2166 - val_accuracy: 0.9670\n",
            "Epoch 590/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0714 - accuracy: 0.9780 - val_loss: 0.2418 - val_accuracy: 0.9670\n",
            "Epoch 591/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0809 - accuracy: 0.9780 - val_loss: 0.2866 - val_accuracy: 0.9560\n",
            "Epoch 592/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9670 - val_loss: 0.1539 - val_accuracy: 0.9780\n",
            "Epoch 593/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0983 - accuracy: 0.9533 - val_loss: 0.1663 - val_accuracy: 0.9670\n",
            "Epoch 594/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0667 - accuracy: 0.9725 - val_loss: 0.2241 - val_accuracy: 0.9560\n",
            "Epoch 595/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9615 - val_loss: 0.1640 - val_accuracy: 0.9780\n",
            "Epoch 596/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.9698 - val_loss: 0.1524 - val_accuracy: 0.9780\n",
            "Epoch 597/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.9725 - val_loss: 0.1516 - val_accuracy: 0.9670\n",
            "Epoch 598/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1088 - accuracy: 0.9588 - val_loss: 0.1483 - val_accuracy: 0.9560\n",
            "Epoch 599/600\n",
            "73/73 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9670 - val_loss: 0.1784 - val_accuracy: 0.9670\n",
            "Epoch 600/600\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.9670 - val_loss: 0.1620 - val_accuracy: 0.9670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-ljjQGWuATk"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1ZALQKEvpNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee46f59-2cbb-4b8e-8fb9-8cb15b4d2a75"
      },
      "source": [
        "loss_value , accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test_loss_value = ' +str(loss_value))\n",
        "acc_CNN=round(accuracy*100,2)\n",
        "print(acc_CNN)\n",
        "\n",
        "print(model.predict(X_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1338 - accuracy: 0.9561\n",
            "Test_loss_value = 0.13380958139896393\n",
            "95.61\n",
            "[[1.5863073e-03]\n",
            " [9.2509246e-01]\n",
            " [1.3509581e-03]\n",
            " [9.9998033e-01]\n",
            " [8.5965854e-01]\n",
            " [9.9996364e-01]\n",
            " [9.9849510e-01]\n",
            " [9.5409852e-01]\n",
            " [3.2497200e-04]\n",
            " [7.7116163e-04]\n",
            " [4.7465201e-05]\n",
            " [8.9187229e-01]\n",
            " [9.8170877e-01]\n",
            " [9.9581701e-04]\n",
            " [5.8481533e-02]\n",
            " [3.0615258e-03]\n",
            " [1.7452656e-05]\n",
            " [2.9264895e-05]\n",
            " [2.2824234e-01]\n",
            " [9.9999988e-01]\n",
            " [9.8272262e-04]\n",
            " [1.3963606e-04]\n",
            " [9.9710113e-01]\n",
            " [6.3404535e-05]\n",
            " [9.8082435e-01]\n",
            " [3.1310268e-02]\n",
            " [5.6982243e-01]\n",
            " [1.0000000e+00]\n",
            " [1.0000000e+00]\n",
            " [9.8456246e-01]\n",
            " [1.0000000e+00]\n",
            " [9.7856762e-05]\n",
            " [1.0000000e+00]\n",
            " [9.9300814e-01]\n",
            " [2.9199966e-04]\n",
            " [7.4032243e-05]\n",
            " [9.9978024e-01]\n",
            " [4.9849745e-02]\n",
            " [2.9855201e-01]\n",
            " [5.3343403e-05]\n",
            " [4.2663037e-06]\n",
            " [5.5439356e-03]\n",
            " [1.6903543e-04]\n",
            " [1.1888700e-02]\n",
            " [1.0800331e-04]\n",
            " [8.5195649e-01]\n",
            " [1.5447838e-02]\n",
            " [9.7460652e-06]\n",
            " [3.3188141e-03]\n",
            " [9.7011101e-01]\n",
            " [9.9999559e-01]\n",
            " [1.0000000e+00]\n",
            " [1.4032026e-02]\n",
            " [1.2955717e-04]\n",
            " [9.3367598e-06]\n",
            " [3.3399634e-04]\n",
            " [4.0430971e-04]\n",
            " [9.9256647e-01]\n",
            " [3.3708164e-03]\n",
            " [1.9650560e-04]\n",
            " [6.5075341e-07]\n",
            " [9.9999368e-01]\n",
            " [1.4248048e-01]\n",
            " [9.5290786e-01]\n",
            " [1.3889800e-02]\n",
            " [5.8457081e-04]\n",
            " [1.8773119e-04]\n",
            " [1.0000000e+00]\n",
            " [8.1906830e-05]\n",
            " [5.3319036e-05]\n",
            " [5.0855000e-05]\n",
            " [3.4577775e-04]\n",
            " [6.2241465e-01]\n",
            " [9.9842036e-01]\n",
            " [7.0604524e-06]\n",
            " [1.0000000e+00]\n",
            " [6.7344898e-01]\n",
            " [4.1649803e-03]\n",
            " [6.9455644e-05]\n",
            " [1.0000000e+00]\n",
            " [1.7664253e-04]\n",
            " [9.9999988e-01]\n",
            " [1.3718747e-05]\n",
            " [9.9998736e-01]\n",
            " [5.3112558e-04]\n",
            " [1.0015689e-05]\n",
            " [8.6200780e-01]\n",
            " [1.6189072e-03]\n",
            " [9.9996078e-01]\n",
            " [2.6711345e-05]\n",
            " [2.5911188e-02]\n",
            " [1.0000000e+00]\n",
            " [3.8549434e-02]\n",
            " [1.8026693e-04]\n",
            " [9.9999249e-01]\n",
            " [9.9819833e-01]\n",
            " [2.2769011e-06]\n",
            " [2.3570657e-03]\n",
            " [1.6783332e-04]\n",
            " [5.6188772e-05]\n",
            " [1.2958859e-03]\n",
            " [2.4792643e-03]\n",
            " [7.7395453e-05]\n",
            " [2.6253739e-02]\n",
            " [4.0945157e-01]\n",
            " [4.7332384e-02]\n",
            " [1.1631452e-04]\n",
            " [1.2451675e-04]\n",
            " [9.9507987e-01]\n",
            " [9.9996769e-01]\n",
            " [9.6404606e-01]\n",
            " [7.8110707e-01]\n",
            " [1.0972026e-01]\n",
            " [5.3203286e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBfRCM2QvpNa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "bbc0e2a4-e533-45ba-9178-f11efcf872a2"
      },
      "source": [
        "\n",
        "def Visualize_Result(accuracy,val_accuracy,loss, val_loss):\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows = 1,\n",
        "                                   ncols = 2,\n",
        "                                   figsize = (15,6),\n",
        "                                   sharex =True)\n",
        "\n",
        "    plot1 = ax1.plot(range(0, len(accuracy)),\n",
        "                     accuracy,\n",
        "                     label = 'accuracy')\n",
        "\n",
        "    plot2 = ax1.plot(range(0, len(val_accuracy)),\n",
        "                     val_accuracy,\n",
        "                     label = 'val_accuracy')\n",
        "\n",
        "    ax1.set(title = 'Accuracy And Val Accuracy progress',\n",
        "            xlabel = 'epoch',\n",
        "            ylabel = 'accuracy/ validation accuracy')\n",
        "\n",
        "    ax1.legend()\n",
        "\n",
        "    plot3 = ax2.plot(range(0, len(loss)),\n",
        "                     loss,\n",
        "                     label = 'loss')\n",
        "    \n",
        "    plot4 = ax2.plot(range(0, len(val_loss)),\n",
        "                     val_loss,\n",
        "                     label = 'val_loss')\n",
        "    \n",
        "    ax2.set(title = 'Loss And Val loss progress',\n",
        "            xlabel = 'epoch',\n",
        "            ylabel = 'loss/ validation loss')\n",
        "\n",
        "    ax2.legend()\n",
        "\n",
        "    fig.suptitle('Result Of Model', fontsize = 20, fontweight = 'bold')\n",
        "    fig.savefig('Accuracy_Loss_figure.png')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_result = Visualize_Result(m.history['accuracy'],m.history['val_accuracy'], m.history['loss'], m.history['val_loss'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKk2BOytvpNc"
      },
      "source": [
        "y_pred=model.predict(X_test)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETHju3q9vpNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e90e7f6d-d36f-47f6-8bef-f62e654fefb5"
      },
      "source": [
        "y_pred.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(114, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksqS_DMtvpNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "febcb14e-758f-4b09-9bef-96ac22a4e3c3"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.5863073e-03],\n",
              "       [9.2509246e-01],\n",
              "       [1.3509581e-03],\n",
              "       [9.9998033e-01],\n",
              "       [8.5965854e-01],\n",
              "       [9.9996364e-01],\n",
              "       [9.9849510e-01],\n",
              "       [9.5409852e-01],\n",
              "       [3.2497200e-04],\n",
              "       [7.7116163e-04],\n",
              "       [4.7465201e-05],\n",
              "       [8.9187229e-01],\n",
              "       [9.8170877e-01],\n",
              "       [9.9581701e-04],\n",
              "       [5.8481533e-02],\n",
              "       [3.0615258e-03],\n",
              "       [1.7452656e-05],\n",
              "       [2.9264895e-05],\n",
              "       [2.2824234e-01],\n",
              "       [9.9999988e-01],\n",
              "       [9.8272262e-04],\n",
              "       [1.3963606e-04],\n",
              "       [9.9710113e-01],\n",
              "       [6.3404535e-05],\n",
              "       [9.8082435e-01],\n",
              "       [3.1310268e-02],\n",
              "       [5.6982243e-01],\n",
              "       [1.0000000e+00],\n",
              "       [1.0000000e+00],\n",
              "       [9.8456246e-01],\n",
              "       [1.0000000e+00],\n",
              "       [9.7856762e-05],\n",
              "       [1.0000000e+00],\n",
              "       [9.9300814e-01],\n",
              "       [2.9199966e-04],\n",
              "       [7.4032243e-05],\n",
              "       [9.9978024e-01],\n",
              "       [4.9849745e-02],\n",
              "       [2.9855201e-01],\n",
              "       [5.3343403e-05],\n",
              "       [4.2663037e-06],\n",
              "       [5.5439356e-03],\n",
              "       [1.6903543e-04],\n",
              "       [1.1888700e-02],\n",
              "       [1.0800331e-04],\n",
              "       [8.5195649e-01],\n",
              "       [1.5447838e-02],\n",
              "       [9.7460652e-06],\n",
              "       [3.3188141e-03],\n",
              "       [9.7011101e-01],\n",
              "       [9.9999559e-01],\n",
              "       [1.0000000e+00],\n",
              "       [1.4032026e-02],\n",
              "       [1.2955717e-04],\n",
              "       [9.3367598e-06],\n",
              "       [3.3399634e-04],\n",
              "       [4.0430971e-04],\n",
              "       [9.9256647e-01],\n",
              "       [3.3708164e-03],\n",
              "       [1.9650560e-04],\n",
              "       [6.5075341e-07],\n",
              "       [9.9999368e-01],\n",
              "       [1.4248048e-01],\n",
              "       [9.5290786e-01],\n",
              "       [1.3889800e-02],\n",
              "       [5.8457081e-04],\n",
              "       [1.8773119e-04],\n",
              "       [1.0000000e+00],\n",
              "       [8.1906830e-05],\n",
              "       [5.3319036e-05],\n",
              "       [5.0855000e-05],\n",
              "       [3.4577775e-04],\n",
              "       [6.2241465e-01],\n",
              "       [9.9842036e-01],\n",
              "       [7.0604524e-06],\n",
              "       [1.0000000e+00],\n",
              "       [6.7344898e-01],\n",
              "       [4.1649803e-03],\n",
              "       [6.9455644e-05],\n",
              "       [1.0000000e+00],\n",
              "       [1.7664253e-04],\n",
              "       [9.9999988e-01],\n",
              "       [1.3718747e-05],\n",
              "       [9.9998736e-01],\n",
              "       [5.3112558e-04],\n",
              "       [1.0015689e-05],\n",
              "       [8.6200780e-01],\n",
              "       [1.6189072e-03],\n",
              "       [9.9996078e-01],\n",
              "       [2.6711345e-05],\n",
              "       [2.5911188e-02],\n",
              "       [1.0000000e+00],\n",
              "       [3.8549434e-02],\n",
              "       [1.8026693e-04],\n",
              "       [9.9999249e-01],\n",
              "       [9.9819833e-01],\n",
              "       [2.2769011e-06],\n",
              "       [2.3570657e-03],\n",
              "       [1.6783332e-04],\n",
              "       [5.6188772e-05],\n",
              "       [1.2958859e-03],\n",
              "       [2.4792643e-03],\n",
              "       [7.7395453e-05],\n",
              "       [2.6253739e-02],\n",
              "       [4.0945157e-01],\n",
              "       [4.7332384e-02],\n",
              "       [1.1631452e-04],\n",
              "       [1.2451675e-04],\n",
              "       [9.9507987e-01],\n",
              "       [9.9996769e-01],\n",
              "       [9.6404606e-01],\n",
              "       [7.8110707e-01],\n",
              "       [1.0972026e-01],\n",
              "       [5.3203286e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcQo5O8mvpNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22894a3-e967-4196-9a21-fe112e32c3cd"
      },
      "source": [
        "np.argmax(model.predict(X_test), axis=-1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t7nq_oj97_V"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdw0ElcSu3Wa"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "```\n",
        "\n",
        "# Fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2M2L8C-xg7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "acbc6838-8c4a-42f6-c6cd-7d04e1feecf4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import sklearn.grid_search\n",
        "import sklearn.metrics\n",
        "import sklearn.neighbors\n",
        "import sklearn.decomposition\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import random \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Data/data.csv\")\n",
        "\n",
        "\n",
        "print(df.describe())\n",
        "\n",
        "X = df.iloc[:,3:12]\n",
        "print(X)\n",
        "Y = df.iloc[:,1]\n",
        "print (X.describe())\n",
        "print (Y.describe())\n",
        "\n",
        "\n",
        "df.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.25,random_state=90)\n",
        "\n",
        "\n",
        "fuzzy = True\n",
        "\n",
        "num_train = len(X_train)\n",
        "num_test  = len(X_test)\n",
        "\n",
        "print (num_test)\n",
        "print (num_train)\n",
        "\n",
        "X_train.head()\n",
        "\n",
        "Y_train.head()\n",
        "\n",
        "X_test.head()\n",
        "\n",
        "Y_test.head()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lc=LabelEncoder()\n",
        "Y_train=lc.fit_transform(Y_train)\n",
        "Y_test=lc.transform(Y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train = X_train.astype(int)\n",
        "X_test = X_test.astype(int)\n",
        "Y_train = Y_train.astype(int)\n",
        "Y_test = Y_test.astype(int)\n",
        "\n",
        "\n",
        "print (X_test)\n",
        "print (Y_test)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "print(confusion_matrix(Y_test, y_pred))\n",
        "acc_fuzzy=round(accuracy_score(Y_test,y_pred)*100,2)\n",
        "print(acc_fuzzy)\n",
        "print(classification_report(Y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "# Calculating accuaracy for different values of K's\n",
        "\n",
        "Ks = 25\n",
        "mean_acc = np.zeros(Ks-1)\n",
        "for n in range(1,Ks):\n",
        "    \n",
        "    #Train Model and Predict  \n",
        "    model_knn = KNeighborsClassifier(n_neighbors = n).fit(X_train,Y_train)\n",
        "    y_hat=model_knn.predict(X_test)\n",
        "    mean_acc[n-1] = accuracy_score(Y_test, y_hat)\n",
        "    print('Accuracy at k =', n, 'is', mean_acc[n-1])\n",
        "\n",
        "\n",
        "mean_acc\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1,Ks),mean_acc,'g')\n",
        "plt.ylabel('Accuracy ')\n",
        "plt.xlabel('Number of neigbours (K)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "error = []\n",
        "\n",
        "# Calculating error for K values between 1 and 40\n",
        "for i in range(1, 40):\n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(X_train, Y_train)\n",
        "    pred_i = knn.predict(X_test)\n",
        "    error.append(np.mean(pred_i != Y_test))\n",
        "\n",
        "error\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n",
        "         markerfacecolor='blue', markersize=10)\n",
        "plt.title('Error Rate K Value')\n",
        "plt.xlabel('K Value')\n",
        "plt.ylabel('Mean Error');"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 id  radius_mean  ...  fractal_dimension_worst  Unnamed: 32\n",
            "count  5.690000e+02   569.000000  ...               569.000000          0.0\n",
            "mean   3.037183e+07    14.127292  ...                 0.083946          NaN\n",
            "std    1.250206e+08     3.524049  ...                 0.018061          NaN\n",
            "min    8.670000e+03     6.981000  ...                 0.055040          NaN\n",
            "25%    8.692180e+05    11.700000  ...                 0.071460          NaN\n",
            "50%    9.060240e+05    13.370000  ...                 0.080040          NaN\n",
            "75%    8.813129e+06    15.780000  ...                 0.092080          NaN\n",
            "max    9.113205e+08    28.110000  ...                 0.207500          NaN\n",
            "\n",
            "[8 rows x 32 columns]\n",
            "     texture_mean  perimeter_mean  ...  symmetry_mean  fractal_dimension_mean\n",
            "0           10.38          122.80  ...         0.2419                 0.07871\n",
            "1           17.77          132.90  ...         0.1812                 0.05667\n",
            "2           21.25          130.00  ...         0.2069                 0.05999\n",
            "3           20.38           77.58  ...         0.2597                 0.09744\n",
            "4           14.34          135.10  ...         0.1809                 0.05883\n",
            "..            ...             ...  ...            ...                     ...\n",
            "564         22.39          142.00  ...         0.1726                 0.05623\n",
            "565         28.25          131.20  ...         0.1752                 0.05533\n",
            "566         28.08          108.30  ...         0.1590                 0.05648\n",
            "567         29.33          140.10  ...         0.2397                 0.07016\n",
            "568         24.54           47.92  ...         0.1587                 0.05884\n",
            "\n",
            "[569 rows x 9 columns]\n",
            "       texture_mean  perimeter_mean  ...  symmetry_mean  fractal_dimension_mean\n",
            "count    569.000000      569.000000  ...     569.000000              569.000000\n",
            "mean      19.289649       91.969033  ...       0.181162                0.062798\n",
            "std        4.301036       24.298981  ...       0.027414                0.007060\n",
            "min        9.710000       43.790000  ...       0.106000                0.049960\n",
            "25%       16.170000       75.170000  ...       0.161900                0.057700\n",
            "50%       18.840000       86.240000  ...       0.179200                0.061540\n",
            "75%       21.800000      104.100000  ...       0.195700                0.066120\n",
            "max       39.280000      188.500000  ...       0.304000                0.097440\n",
            "\n",
            "[8 rows x 9 columns]\n",
            "count     569\n",
            "unique      2\n",
            "top         B\n",
            "freq      357\n",
            "Name: diagnosis, dtype: object\n",
            "143\n",
            "426\n",
            "     texture_mean  perimeter_mean  ...  symmetry_mean  fractal_dimension_mean\n",
            "334            19              77  ...              0                       0\n",
            "490            22              78  ...              0                       0\n",
            "418            12              80  ...              0                       0\n",
            "224            17              84  ...              0                       0\n",
            "151            20              53  ...              0                       0\n",
            "..            ...             ...  ...            ...                     ...\n",
            "362            18              81  ...              0                       0\n",
            "489            20             107  ...              0                       0\n",
            "318            18              60  ...              0                       0\n",
            "470            18              61  ...              0                       0\n",
            "256            28             133  ...              0                       0\n",
            "\n",
            "[143 rows x 9 columns]\n",
            "[0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1]\n",
            "[[82  9]\n",
            " [ 7 45]]\n",
            "88.81\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.90      0.91        91\n",
            "           1       0.83      0.87      0.85        52\n",
            "\n",
            "    accuracy                           0.89       143\n",
            "   macro avg       0.88      0.88      0.88       143\n",
            "weighted avg       0.89      0.89      0.89       143\n",
            "\n",
            "Accuracy at k = 1 is 0.8951048951048951\n",
            "Accuracy at k = 2 is 0.8951048951048951\n",
            "Accuracy at k = 3 is 0.9020979020979021\n",
            "Accuracy at k = 4 is 0.9020979020979021\n",
            "Accuracy at k = 5 is 0.8881118881118881\n",
            "Accuracy at k = 6 is 0.9020979020979021\n",
            "Accuracy at k = 7 is 0.8881118881118881\n",
            "Accuracy at k = 8 is 0.8951048951048951\n",
            "Accuracy at k = 9 is 0.8951048951048951\n",
            "Accuracy at k = 10 is 0.8881118881118881\n",
            "Accuracy at k = 11 is 0.8811188811188811\n",
            "Accuracy at k = 12 is 0.9090909090909091\n",
            "Accuracy at k = 13 is 0.9090909090909091\n",
            "Accuracy at k = 14 is 0.9090909090909091\n",
            "Accuracy at k = 15 is 0.9090909090909091\n",
            "Accuracy at k = 16 is 0.916083916083916\n",
            "Accuracy at k = 17 is 0.916083916083916\n",
            "Accuracy at k = 18 is 0.916083916083916\n",
            "Accuracy at k = 19 is 0.916083916083916\n",
            "Accuracy at k = 20 is 0.916083916083916\n",
            "Accuracy at k = 21 is 0.916083916083916\n",
            "Accuracy at k = 22 is 0.916083916083916\n",
            "Accuracy at k = 23 is 0.916083916083916\n",
            "Accuracy at k = 24 is 0.916083916083916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6yU6V-MxkkO"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUwXDx90lNuE"
      },
      "source": [
        "# Recurrent neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJtxXpaumXcd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryU7gNRjlWGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "457a92a4-444c-43ea-d1fb-374c8f2624e7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import AveragePooling1D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import random # for visualization\n",
        "print('Libraries Imported')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Libraries Imported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhzTrhuqs6s_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0123886e-b79c-40aa-b007-cae8ea0678e6"
      },
      "source": [
        "info=pd.read_csv(\"/content/drive/My Drive/Data/data.csv\")\n",
        "print(info.head(5))\n",
        "print(info.info())\n",
        "print(info.describe())\n",
        "\n",
        "print(info.isna().sum())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
            "0    842302         M  ...                  0.11890          NaN\n",
            "1    842517         M  ...                  0.08902          NaN\n",
            "2  84300903         M  ...                  0.08758          NaN\n",
            "3  84348301         M  ...                  0.17300          NaN\n",
            "4  84358402         M  ...                  0.07678          NaN\n",
            "\n",
            "[5 rows x 33 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 33 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   id                       569 non-null    int64  \n",
            " 1   diagnosis                569 non-null    object \n",
            " 2   radius_mean              569 non-null    float64\n",
            " 3   texture_mean             569 non-null    float64\n",
            " 4   perimeter_mean           569 non-null    float64\n",
            " 5   area_mean                569 non-null    float64\n",
            " 6   smoothness_mean          569 non-null    float64\n",
            " 7   compactness_mean         569 non-null    float64\n",
            " 8   concavity_mean           569 non-null    float64\n",
            " 9   concave points_mean      569 non-null    float64\n",
            " 10  symmetry_mean            569 non-null    float64\n",
            " 11  fractal_dimension_mean   569 non-null    float64\n",
            " 12  radius_se                569 non-null    float64\n",
            " 13  texture_se               569 non-null    float64\n",
            " 14  perimeter_se             569 non-null    float64\n",
            " 15  area_se                  569 non-null    float64\n",
            " 16  smoothness_se            569 non-null    float64\n",
            " 17  compactness_se           569 non-null    float64\n",
            " 18  concavity_se             569 non-null    float64\n",
            " 19  concave points_se        569 non-null    float64\n",
            " 20  symmetry_se              569 non-null    float64\n",
            " 21  fractal_dimension_se     569 non-null    float64\n",
            " 22  radius_worst             569 non-null    float64\n",
            " 23  texture_worst            569 non-null    float64\n",
            " 24  perimeter_worst          569 non-null    float64\n",
            " 25  area_worst               569 non-null    float64\n",
            " 26  smoothness_worst         569 non-null    float64\n",
            " 27  compactness_worst        569 non-null    float64\n",
            " 28  concavity_worst          569 non-null    float64\n",
            " 29  concave points_worst     569 non-null    float64\n",
            " 30  symmetry_worst           569 non-null    float64\n",
            " 31  fractal_dimension_worst  569 non-null    float64\n",
            " 32  Unnamed: 32              0 non-null      float64\n",
            "dtypes: float64(31), int64(1), object(1)\n",
            "memory usage: 146.8+ KB\n",
            "None\n",
            "                 id  radius_mean  ...  fractal_dimension_worst  Unnamed: 32\n",
            "count  5.690000e+02   569.000000  ...               569.000000          0.0\n",
            "mean   3.037183e+07    14.127292  ...                 0.083946          NaN\n",
            "std    1.250206e+08     3.524049  ...                 0.018061          NaN\n",
            "min    8.670000e+03     6.981000  ...                 0.055040          NaN\n",
            "25%    8.692180e+05    11.700000  ...                 0.071460          NaN\n",
            "50%    9.060240e+05    13.370000  ...                 0.080040          NaN\n",
            "75%    8.813129e+06    15.780000  ...                 0.092080          NaN\n",
            "max    9.113205e+08    28.110000  ...                 0.207500          NaN\n",
            "\n",
            "[8 rows x 32 columns]\n",
            "id                           0\n",
            "diagnosis                    0\n",
            "radius_mean                  0\n",
            "texture_mean                 0\n",
            "perimeter_mean               0\n",
            "area_mean                    0\n",
            "smoothness_mean              0\n",
            "compactness_mean             0\n",
            "concavity_mean               0\n",
            "concave points_mean          0\n",
            "symmetry_mean                0\n",
            "fractal_dimension_mean       0\n",
            "radius_se                    0\n",
            "texture_se                   0\n",
            "perimeter_se                 0\n",
            "area_se                      0\n",
            "smoothness_se                0\n",
            "compactness_se               0\n",
            "concavity_se                 0\n",
            "concave points_se            0\n",
            "symmetry_se                  0\n",
            "fractal_dimension_se         0\n",
            "radius_worst                 0\n",
            "texture_worst                0\n",
            "perimeter_worst              0\n",
            "area_worst                   0\n",
            "smoothness_worst             0\n",
            "compactness_worst            0\n",
            "concavity_worst              0\n",
            "concave points_worst         0\n",
            "symmetry_worst               0\n",
            "fractal_dimension_worst      0\n",
            "Unnamed: 32                569\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoq-tWQilTAi"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "info['diagnosis']=le.fit_transform(info['diagnosis'])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u1l8TXZ0LEcJ",
        "outputId": "a85d94c2-43e3-4be8-deff-b604e8ee2b1b"
      },
      "source": [
        "info.corr()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.039769</td>\n",
              "      <td>0.074626</td>\n",
              "      <td>0.099770</td>\n",
              "      <td>0.073159</td>\n",
              "      <td>0.096893</td>\n",
              "      <td>-0.012968</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.050080</td>\n",
              "      <td>0.044158</td>\n",
              "      <td>-0.022114</td>\n",
              "      <td>-0.052511</td>\n",
              "      <td>0.143048</td>\n",
              "      <td>-0.007526</td>\n",
              "      <td>0.137331</td>\n",
              "      <td>0.177742</td>\n",
              "      <td>0.096781</td>\n",
              "      <td>0.033961</td>\n",
              "      <td>0.055239</td>\n",
              "      <td>0.078768</td>\n",
              "      <td>-0.017306</td>\n",
              "      <td>0.025725</td>\n",
              "      <td>0.082405</td>\n",
              "      <td>0.064720</td>\n",
              "      <td>0.079986</td>\n",
              "      <td>0.107187</td>\n",
              "      <td>0.010338</td>\n",
              "      <td>-0.002968</td>\n",
              "      <td>0.023203</td>\n",
              "      <td>0.035174</td>\n",
              "      <td>-0.044224</td>\n",
              "      <td>-0.029866</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>diagnosis</th>\n",
              "      <td>0.039769</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.730029</td>\n",
              "      <td>0.415185</td>\n",
              "      <td>0.742636</td>\n",
              "      <td>0.708984</td>\n",
              "      <td>0.358560</td>\n",
              "      <td>0.596534</td>\n",
              "      <td>0.696360</td>\n",
              "      <td>0.776614</td>\n",
              "      <td>0.330499</td>\n",
              "      <td>-0.012838</td>\n",
              "      <td>0.567134</td>\n",
              "      <td>-0.008303</td>\n",
              "      <td>0.556141</td>\n",
              "      <td>0.548236</td>\n",
              "      <td>-0.067016</td>\n",
              "      <td>0.292999</td>\n",
              "      <td>0.253730</td>\n",
              "      <td>0.408042</td>\n",
              "      <td>-0.006522</td>\n",
              "      <td>0.077972</td>\n",
              "      <td>0.776454</td>\n",
              "      <td>0.456903</td>\n",
              "      <td>0.782914</td>\n",
              "      <td>0.733825</td>\n",
              "      <td>0.421465</td>\n",
              "      <td>0.590998</td>\n",
              "      <td>0.659610</td>\n",
              "      <td>0.793566</td>\n",
              "      <td>0.416294</td>\n",
              "      <td>0.323872</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_mean</th>\n",
              "      <td>0.074626</td>\n",
              "      <td>0.730029</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.323782</td>\n",
              "      <td>0.997855</td>\n",
              "      <td>0.987357</td>\n",
              "      <td>0.170581</td>\n",
              "      <td>0.506124</td>\n",
              "      <td>0.676764</td>\n",
              "      <td>0.822529</td>\n",
              "      <td>0.147741</td>\n",
              "      <td>-0.311631</td>\n",
              "      <td>0.679090</td>\n",
              "      <td>-0.097317</td>\n",
              "      <td>0.674172</td>\n",
              "      <td>0.735864</td>\n",
              "      <td>-0.222600</td>\n",
              "      <td>0.206000</td>\n",
              "      <td>0.194204</td>\n",
              "      <td>0.376169</td>\n",
              "      <td>-0.104321</td>\n",
              "      <td>-0.042641</td>\n",
              "      <td>0.969539</td>\n",
              "      <td>0.297008</td>\n",
              "      <td>0.965137</td>\n",
              "      <td>0.941082</td>\n",
              "      <td>0.119616</td>\n",
              "      <td>0.413463</td>\n",
              "      <td>0.526911</td>\n",
              "      <td>0.744214</td>\n",
              "      <td>0.163953</td>\n",
              "      <td>0.007066</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_mean</th>\n",
              "      <td>0.099770</td>\n",
              "      <td>0.415185</td>\n",
              "      <td>0.323782</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.329533</td>\n",
              "      <td>0.321086</td>\n",
              "      <td>-0.023389</td>\n",
              "      <td>0.236702</td>\n",
              "      <td>0.302418</td>\n",
              "      <td>0.293464</td>\n",
              "      <td>0.071401</td>\n",
              "      <td>-0.076437</td>\n",
              "      <td>0.275869</td>\n",
              "      <td>0.386358</td>\n",
              "      <td>0.281673</td>\n",
              "      <td>0.259845</td>\n",
              "      <td>0.006614</td>\n",
              "      <td>0.191975</td>\n",
              "      <td>0.143293</td>\n",
              "      <td>0.163851</td>\n",
              "      <td>0.009127</td>\n",
              "      <td>0.054458</td>\n",
              "      <td>0.352573</td>\n",
              "      <td>0.912045</td>\n",
              "      <td>0.358040</td>\n",
              "      <td>0.343546</td>\n",
              "      <td>0.077503</td>\n",
              "      <td>0.277830</td>\n",
              "      <td>0.301025</td>\n",
              "      <td>0.295316</td>\n",
              "      <td>0.105008</td>\n",
              "      <td>0.119205</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_mean</th>\n",
              "      <td>0.073159</td>\n",
              "      <td>0.742636</td>\n",
              "      <td>0.997855</td>\n",
              "      <td>0.329533</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.986507</td>\n",
              "      <td>0.207278</td>\n",
              "      <td>0.556936</td>\n",
              "      <td>0.716136</td>\n",
              "      <td>0.850977</td>\n",
              "      <td>0.183027</td>\n",
              "      <td>-0.261477</td>\n",
              "      <td>0.691765</td>\n",
              "      <td>-0.086761</td>\n",
              "      <td>0.693135</td>\n",
              "      <td>0.744983</td>\n",
              "      <td>-0.202694</td>\n",
              "      <td>0.250744</td>\n",
              "      <td>0.228082</td>\n",
              "      <td>0.407217</td>\n",
              "      <td>-0.081629</td>\n",
              "      <td>-0.005523</td>\n",
              "      <td>0.969476</td>\n",
              "      <td>0.303038</td>\n",
              "      <td>0.970387</td>\n",
              "      <td>0.941550</td>\n",
              "      <td>0.150549</td>\n",
              "      <td>0.455774</td>\n",
              "      <td>0.563879</td>\n",
              "      <td>0.771241</td>\n",
              "      <td>0.189115</td>\n",
              "      <td>0.051019</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_mean</th>\n",
              "      <td>0.096893</td>\n",
              "      <td>0.708984</td>\n",
              "      <td>0.987357</td>\n",
              "      <td>0.321086</td>\n",
              "      <td>0.986507</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.177028</td>\n",
              "      <td>0.498502</td>\n",
              "      <td>0.685983</td>\n",
              "      <td>0.823269</td>\n",
              "      <td>0.151293</td>\n",
              "      <td>-0.283110</td>\n",
              "      <td>0.732562</td>\n",
              "      <td>-0.066280</td>\n",
              "      <td>0.726628</td>\n",
              "      <td>0.800086</td>\n",
              "      <td>-0.166777</td>\n",
              "      <td>0.212583</td>\n",
              "      <td>0.207660</td>\n",
              "      <td>0.372320</td>\n",
              "      <td>-0.072497</td>\n",
              "      <td>-0.019887</td>\n",
              "      <td>0.962746</td>\n",
              "      <td>0.287489</td>\n",
              "      <td>0.959120</td>\n",
              "      <td>0.959213</td>\n",
              "      <td>0.123523</td>\n",
              "      <td>0.390410</td>\n",
              "      <td>0.512606</td>\n",
              "      <td>0.722017</td>\n",
              "      <td>0.143570</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_mean</th>\n",
              "      <td>-0.012968</td>\n",
              "      <td>0.358560</td>\n",
              "      <td>0.170581</td>\n",
              "      <td>-0.023389</td>\n",
              "      <td>0.207278</td>\n",
              "      <td>0.177028</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.659123</td>\n",
              "      <td>0.521984</td>\n",
              "      <td>0.553695</td>\n",
              "      <td>0.557775</td>\n",
              "      <td>0.584792</td>\n",
              "      <td>0.301467</td>\n",
              "      <td>0.068406</td>\n",
              "      <td>0.296092</td>\n",
              "      <td>0.246552</td>\n",
              "      <td>0.332375</td>\n",
              "      <td>0.318943</td>\n",
              "      <td>0.248396</td>\n",
              "      <td>0.380676</td>\n",
              "      <td>0.200774</td>\n",
              "      <td>0.283607</td>\n",
              "      <td>0.213120</td>\n",
              "      <td>0.036072</td>\n",
              "      <td>0.238853</td>\n",
              "      <td>0.206718</td>\n",
              "      <td>0.805324</td>\n",
              "      <td>0.472468</td>\n",
              "      <td>0.434926</td>\n",
              "      <td>0.503053</td>\n",
              "      <td>0.394309</td>\n",
              "      <td>0.499316</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_mean</th>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.596534</td>\n",
              "      <td>0.506124</td>\n",
              "      <td>0.236702</td>\n",
              "      <td>0.556936</td>\n",
              "      <td>0.498502</td>\n",
              "      <td>0.659123</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.883121</td>\n",
              "      <td>0.831135</td>\n",
              "      <td>0.602641</td>\n",
              "      <td>0.565369</td>\n",
              "      <td>0.497473</td>\n",
              "      <td>0.046205</td>\n",
              "      <td>0.548905</td>\n",
              "      <td>0.455653</td>\n",
              "      <td>0.135299</td>\n",
              "      <td>0.738722</td>\n",
              "      <td>0.570517</td>\n",
              "      <td>0.642262</td>\n",
              "      <td>0.229977</td>\n",
              "      <td>0.507318</td>\n",
              "      <td>0.535315</td>\n",
              "      <td>0.248133</td>\n",
              "      <td>0.590210</td>\n",
              "      <td>0.509604</td>\n",
              "      <td>0.565541</td>\n",
              "      <td>0.865809</td>\n",
              "      <td>0.816275</td>\n",
              "      <td>0.815573</td>\n",
              "      <td>0.510223</td>\n",
              "      <td>0.687382</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_mean</th>\n",
              "      <td>0.050080</td>\n",
              "      <td>0.696360</td>\n",
              "      <td>0.676764</td>\n",
              "      <td>0.302418</td>\n",
              "      <td>0.716136</td>\n",
              "      <td>0.685983</td>\n",
              "      <td>0.521984</td>\n",
              "      <td>0.883121</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.921391</td>\n",
              "      <td>0.500667</td>\n",
              "      <td>0.336783</td>\n",
              "      <td>0.631925</td>\n",
              "      <td>0.076218</td>\n",
              "      <td>0.660391</td>\n",
              "      <td>0.617427</td>\n",
              "      <td>0.098564</td>\n",
              "      <td>0.670279</td>\n",
              "      <td>0.691270</td>\n",
              "      <td>0.683260</td>\n",
              "      <td>0.178009</td>\n",
              "      <td>0.449301</td>\n",
              "      <td>0.688236</td>\n",
              "      <td>0.299879</td>\n",
              "      <td>0.729565</td>\n",
              "      <td>0.675987</td>\n",
              "      <td>0.448822</td>\n",
              "      <td>0.754968</td>\n",
              "      <td>0.884103</td>\n",
              "      <td>0.861323</td>\n",
              "      <td>0.409464</td>\n",
              "      <td>0.514930</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points_mean</th>\n",
              "      <td>0.044158</td>\n",
              "      <td>0.776614</td>\n",
              "      <td>0.822529</td>\n",
              "      <td>0.293464</td>\n",
              "      <td>0.850977</td>\n",
              "      <td>0.823269</td>\n",
              "      <td>0.553695</td>\n",
              "      <td>0.831135</td>\n",
              "      <td>0.921391</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.462497</td>\n",
              "      <td>0.166917</td>\n",
              "      <td>0.698050</td>\n",
              "      <td>0.021480</td>\n",
              "      <td>0.710650</td>\n",
              "      <td>0.690299</td>\n",
              "      <td>0.027653</td>\n",
              "      <td>0.490424</td>\n",
              "      <td>0.439167</td>\n",
              "      <td>0.615634</td>\n",
              "      <td>0.095351</td>\n",
              "      <td>0.257584</td>\n",
              "      <td>0.830318</td>\n",
              "      <td>0.292752</td>\n",
              "      <td>0.855923</td>\n",
              "      <td>0.809630</td>\n",
              "      <td>0.452753</td>\n",
              "      <td>0.667454</td>\n",
              "      <td>0.752399</td>\n",
              "      <td>0.910155</td>\n",
              "      <td>0.375744</td>\n",
              "      <td>0.368661</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_mean</th>\n",
              "      <td>-0.022114</td>\n",
              "      <td>0.330499</td>\n",
              "      <td>0.147741</td>\n",
              "      <td>0.071401</td>\n",
              "      <td>0.183027</td>\n",
              "      <td>0.151293</td>\n",
              "      <td>0.557775</td>\n",
              "      <td>0.602641</td>\n",
              "      <td>0.500667</td>\n",
              "      <td>0.462497</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.479921</td>\n",
              "      <td>0.303379</td>\n",
              "      <td>0.128053</td>\n",
              "      <td>0.313893</td>\n",
              "      <td>0.223970</td>\n",
              "      <td>0.187321</td>\n",
              "      <td>0.421659</td>\n",
              "      <td>0.342627</td>\n",
              "      <td>0.393298</td>\n",
              "      <td>0.449137</td>\n",
              "      <td>0.331786</td>\n",
              "      <td>0.185728</td>\n",
              "      <td>0.090651</td>\n",
              "      <td>0.219169</td>\n",
              "      <td>0.177193</td>\n",
              "      <td>0.426675</td>\n",
              "      <td>0.473200</td>\n",
              "      <td>0.433721</td>\n",
              "      <td>0.430297</td>\n",
              "      <td>0.699826</td>\n",
              "      <td>0.438413</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <td>-0.052511</td>\n",
              "      <td>-0.012838</td>\n",
              "      <td>-0.311631</td>\n",
              "      <td>-0.076437</td>\n",
              "      <td>-0.261477</td>\n",
              "      <td>-0.283110</td>\n",
              "      <td>0.584792</td>\n",
              "      <td>0.565369</td>\n",
              "      <td>0.336783</td>\n",
              "      <td>0.166917</td>\n",
              "      <td>0.479921</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.164174</td>\n",
              "      <td>0.039830</td>\n",
              "      <td>-0.090170</td>\n",
              "      <td>0.401964</td>\n",
              "      <td>0.559837</td>\n",
              "      <td>0.446630</td>\n",
              "      <td>0.341198</td>\n",
              "      <td>0.345007</td>\n",
              "      <td>0.688132</td>\n",
              "      <td>-0.253691</td>\n",
              "      <td>-0.051269</td>\n",
              "      <td>-0.205151</td>\n",
              "      <td>-0.231854</td>\n",
              "      <td>0.504942</td>\n",
              "      <td>0.458798</td>\n",
              "      <td>0.346234</td>\n",
              "      <td>0.175325</td>\n",
              "      <td>0.334019</td>\n",
              "      <td>0.767297</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_se</th>\n",
              "      <td>0.143048</td>\n",
              "      <td>0.567134</td>\n",
              "      <td>0.679090</td>\n",
              "      <td>0.275869</td>\n",
              "      <td>0.691765</td>\n",
              "      <td>0.732562</td>\n",
              "      <td>0.301467</td>\n",
              "      <td>0.497473</td>\n",
              "      <td>0.631925</td>\n",
              "      <td>0.698050</td>\n",
              "      <td>0.303379</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>0.972794</td>\n",
              "      <td>0.951830</td>\n",
              "      <td>0.164514</td>\n",
              "      <td>0.356065</td>\n",
              "      <td>0.332358</td>\n",
              "      <td>0.513346</td>\n",
              "      <td>0.240567</td>\n",
              "      <td>0.227754</td>\n",
              "      <td>0.715065</td>\n",
              "      <td>0.194799</td>\n",
              "      <td>0.719684</td>\n",
              "      <td>0.751548</td>\n",
              "      <td>0.141919</td>\n",
              "      <td>0.287103</td>\n",
              "      <td>0.380585</td>\n",
              "      <td>0.531062</td>\n",
              "      <td>0.094543</td>\n",
              "      <td>0.049559</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_se</th>\n",
              "      <td>-0.007526</td>\n",
              "      <td>-0.008303</td>\n",
              "      <td>-0.097317</td>\n",
              "      <td>0.386358</td>\n",
              "      <td>-0.086761</td>\n",
              "      <td>-0.066280</td>\n",
              "      <td>0.068406</td>\n",
              "      <td>0.046205</td>\n",
              "      <td>0.076218</td>\n",
              "      <td>0.021480</td>\n",
              "      <td>0.128053</td>\n",
              "      <td>0.164174</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.223171</td>\n",
              "      <td>0.111567</td>\n",
              "      <td>0.397243</td>\n",
              "      <td>0.231700</td>\n",
              "      <td>0.194998</td>\n",
              "      <td>0.230283</td>\n",
              "      <td>0.411621</td>\n",
              "      <td>0.279723</td>\n",
              "      <td>-0.111690</td>\n",
              "      <td>0.409003</td>\n",
              "      <td>-0.102242</td>\n",
              "      <td>-0.083195</td>\n",
              "      <td>-0.073658</td>\n",
              "      <td>-0.092439</td>\n",
              "      <td>-0.068956</td>\n",
              "      <td>-0.119638</td>\n",
              "      <td>-0.128215</td>\n",
              "      <td>-0.045655</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_se</th>\n",
              "      <td>0.137331</td>\n",
              "      <td>0.556141</td>\n",
              "      <td>0.674172</td>\n",
              "      <td>0.281673</td>\n",
              "      <td>0.693135</td>\n",
              "      <td>0.726628</td>\n",
              "      <td>0.296092</td>\n",
              "      <td>0.548905</td>\n",
              "      <td>0.660391</td>\n",
              "      <td>0.710650</td>\n",
              "      <td>0.313893</td>\n",
              "      <td>0.039830</td>\n",
              "      <td>0.972794</td>\n",
              "      <td>0.223171</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.937655</td>\n",
              "      <td>0.151075</td>\n",
              "      <td>0.416322</td>\n",
              "      <td>0.362482</td>\n",
              "      <td>0.556264</td>\n",
              "      <td>0.266487</td>\n",
              "      <td>0.244143</td>\n",
              "      <td>0.697201</td>\n",
              "      <td>0.200371</td>\n",
              "      <td>0.721031</td>\n",
              "      <td>0.730713</td>\n",
              "      <td>0.130054</td>\n",
              "      <td>0.341919</td>\n",
              "      <td>0.418899</td>\n",
              "      <td>0.554897</td>\n",
              "      <td>0.109930</td>\n",
              "      <td>0.085433</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_se</th>\n",
              "      <td>0.177742</td>\n",
              "      <td>0.548236</td>\n",
              "      <td>0.735864</td>\n",
              "      <td>0.259845</td>\n",
              "      <td>0.744983</td>\n",
              "      <td>0.800086</td>\n",
              "      <td>0.246552</td>\n",
              "      <td>0.455653</td>\n",
              "      <td>0.617427</td>\n",
              "      <td>0.690299</td>\n",
              "      <td>0.223970</td>\n",
              "      <td>-0.090170</td>\n",
              "      <td>0.951830</td>\n",
              "      <td>0.111567</td>\n",
              "      <td>0.937655</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.075150</td>\n",
              "      <td>0.284840</td>\n",
              "      <td>0.270895</td>\n",
              "      <td>0.415730</td>\n",
              "      <td>0.134109</td>\n",
              "      <td>0.127071</td>\n",
              "      <td>0.757373</td>\n",
              "      <td>0.196497</td>\n",
              "      <td>0.761213</td>\n",
              "      <td>0.811408</td>\n",
              "      <td>0.125389</td>\n",
              "      <td>0.283257</td>\n",
              "      <td>0.385100</td>\n",
              "      <td>0.538166</td>\n",
              "      <td>0.074126</td>\n",
              "      <td>0.017539</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_se</th>\n",
              "      <td>0.096781</td>\n",
              "      <td>-0.067016</td>\n",
              "      <td>-0.222600</td>\n",
              "      <td>0.006614</td>\n",
              "      <td>-0.202694</td>\n",
              "      <td>-0.166777</td>\n",
              "      <td>0.332375</td>\n",
              "      <td>0.135299</td>\n",
              "      <td>0.098564</td>\n",
              "      <td>0.027653</td>\n",
              "      <td>0.187321</td>\n",
              "      <td>0.401964</td>\n",
              "      <td>0.164514</td>\n",
              "      <td>0.397243</td>\n",
              "      <td>0.151075</td>\n",
              "      <td>0.075150</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.336696</td>\n",
              "      <td>0.268685</td>\n",
              "      <td>0.328429</td>\n",
              "      <td>0.413506</td>\n",
              "      <td>0.427374</td>\n",
              "      <td>-0.230691</td>\n",
              "      <td>-0.074743</td>\n",
              "      <td>-0.217304</td>\n",
              "      <td>-0.182195</td>\n",
              "      <td>0.314457</td>\n",
              "      <td>-0.055558</td>\n",
              "      <td>-0.058298</td>\n",
              "      <td>-0.102007</td>\n",
              "      <td>-0.107342</td>\n",
              "      <td>0.101480</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_se</th>\n",
              "      <td>0.033961</td>\n",
              "      <td>0.292999</td>\n",
              "      <td>0.206000</td>\n",
              "      <td>0.191975</td>\n",
              "      <td>0.250744</td>\n",
              "      <td>0.212583</td>\n",
              "      <td>0.318943</td>\n",
              "      <td>0.738722</td>\n",
              "      <td>0.670279</td>\n",
              "      <td>0.490424</td>\n",
              "      <td>0.421659</td>\n",
              "      <td>0.559837</td>\n",
              "      <td>0.356065</td>\n",
              "      <td>0.231700</td>\n",
              "      <td>0.416322</td>\n",
              "      <td>0.284840</td>\n",
              "      <td>0.336696</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.801268</td>\n",
              "      <td>0.744083</td>\n",
              "      <td>0.394713</td>\n",
              "      <td>0.803269</td>\n",
              "      <td>0.204607</td>\n",
              "      <td>0.143003</td>\n",
              "      <td>0.260516</td>\n",
              "      <td>0.199371</td>\n",
              "      <td>0.227394</td>\n",
              "      <td>0.678780</td>\n",
              "      <td>0.639147</td>\n",
              "      <td>0.483208</td>\n",
              "      <td>0.277878</td>\n",
              "      <td>0.590973</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_se</th>\n",
              "      <td>0.055239</td>\n",
              "      <td>0.253730</td>\n",
              "      <td>0.194204</td>\n",
              "      <td>0.143293</td>\n",
              "      <td>0.228082</td>\n",
              "      <td>0.207660</td>\n",
              "      <td>0.248396</td>\n",
              "      <td>0.570517</td>\n",
              "      <td>0.691270</td>\n",
              "      <td>0.439167</td>\n",
              "      <td>0.342627</td>\n",
              "      <td>0.446630</td>\n",
              "      <td>0.332358</td>\n",
              "      <td>0.194998</td>\n",
              "      <td>0.362482</td>\n",
              "      <td>0.270895</td>\n",
              "      <td>0.268685</td>\n",
              "      <td>0.801268</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.771804</td>\n",
              "      <td>0.309429</td>\n",
              "      <td>0.727372</td>\n",
              "      <td>0.186904</td>\n",
              "      <td>0.100241</td>\n",
              "      <td>0.226680</td>\n",
              "      <td>0.188353</td>\n",
              "      <td>0.168481</td>\n",
              "      <td>0.484858</td>\n",
              "      <td>0.662564</td>\n",
              "      <td>0.440472</td>\n",
              "      <td>0.197788</td>\n",
              "      <td>0.439329</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points_se</th>\n",
              "      <td>0.078768</td>\n",
              "      <td>0.408042</td>\n",
              "      <td>0.376169</td>\n",
              "      <td>0.163851</td>\n",
              "      <td>0.407217</td>\n",
              "      <td>0.372320</td>\n",
              "      <td>0.380676</td>\n",
              "      <td>0.642262</td>\n",
              "      <td>0.683260</td>\n",
              "      <td>0.615634</td>\n",
              "      <td>0.393298</td>\n",
              "      <td>0.341198</td>\n",
              "      <td>0.513346</td>\n",
              "      <td>0.230283</td>\n",
              "      <td>0.556264</td>\n",
              "      <td>0.415730</td>\n",
              "      <td>0.328429</td>\n",
              "      <td>0.744083</td>\n",
              "      <td>0.771804</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.312780</td>\n",
              "      <td>0.611044</td>\n",
              "      <td>0.358127</td>\n",
              "      <td>0.086741</td>\n",
              "      <td>0.394999</td>\n",
              "      <td>0.342271</td>\n",
              "      <td>0.215351</td>\n",
              "      <td>0.452888</td>\n",
              "      <td>0.549592</td>\n",
              "      <td>0.602450</td>\n",
              "      <td>0.143116</td>\n",
              "      <td>0.310655</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_se</th>\n",
              "      <td>-0.017306</td>\n",
              "      <td>-0.006522</td>\n",
              "      <td>-0.104321</td>\n",
              "      <td>0.009127</td>\n",
              "      <td>-0.081629</td>\n",
              "      <td>-0.072497</td>\n",
              "      <td>0.200774</td>\n",
              "      <td>0.229977</td>\n",
              "      <td>0.178009</td>\n",
              "      <td>0.095351</td>\n",
              "      <td>0.449137</td>\n",
              "      <td>0.345007</td>\n",
              "      <td>0.240567</td>\n",
              "      <td>0.411621</td>\n",
              "      <td>0.266487</td>\n",
              "      <td>0.134109</td>\n",
              "      <td>0.413506</td>\n",
              "      <td>0.394713</td>\n",
              "      <td>0.309429</td>\n",
              "      <td>0.312780</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.369078</td>\n",
              "      <td>-0.128121</td>\n",
              "      <td>-0.077473</td>\n",
              "      <td>-0.103753</td>\n",
              "      <td>-0.110343</td>\n",
              "      <td>-0.012662</td>\n",
              "      <td>0.060255</td>\n",
              "      <td>0.037119</td>\n",
              "      <td>-0.030413</td>\n",
              "      <td>0.389402</td>\n",
              "      <td>0.078079</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <td>0.025725</td>\n",
              "      <td>0.077972</td>\n",
              "      <td>-0.042641</td>\n",
              "      <td>0.054458</td>\n",
              "      <td>-0.005523</td>\n",
              "      <td>-0.019887</td>\n",
              "      <td>0.283607</td>\n",
              "      <td>0.507318</td>\n",
              "      <td>0.449301</td>\n",
              "      <td>0.257584</td>\n",
              "      <td>0.331786</td>\n",
              "      <td>0.688132</td>\n",
              "      <td>0.227754</td>\n",
              "      <td>0.279723</td>\n",
              "      <td>0.244143</td>\n",
              "      <td>0.127071</td>\n",
              "      <td>0.427374</td>\n",
              "      <td>0.803269</td>\n",
              "      <td>0.727372</td>\n",
              "      <td>0.611044</td>\n",
              "      <td>0.369078</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.037488</td>\n",
              "      <td>-0.003195</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>-0.022736</td>\n",
              "      <td>0.170568</td>\n",
              "      <td>0.390159</td>\n",
              "      <td>0.379975</td>\n",
              "      <td>0.215204</td>\n",
              "      <td>0.111094</td>\n",
              "      <td>0.591328</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_worst</th>\n",
              "      <td>0.082405</td>\n",
              "      <td>0.776454</td>\n",
              "      <td>0.969539</td>\n",
              "      <td>0.352573</td>\n",
              "      <td>0.969476</td>\n",
              "      <td>0.962746</td>\n",
              "      <td>0.213120</td>\n",
              "      <td>0.535315</td>\n",
              "      <td>0.688236</td>\n",
              "      <td>0.830318</td>\n",
              "      <td>0.185728</td>\n",
              "      <td>-0.253691</td>\n",
              "      <td>0.715065</td>\n",
              "      <td>-0.111690</td>\n",
              "      <td>0.697201</td>\n",
              "      <td>0.757373</td>\n",
              "      <td>-0.230691</td>\n",
              "      <td>0.204607</td>\n",
              "      <td>0.186904</td>\n",
              "      <td>0.358127</td>\n",
              "      <td>-0.128121</td>\n",
              "      <td>-0.037488</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.359921</td>\n",
              "      <td>0.993708</td>\n",
              "      <td>0.984015</td>\n",
              "      <td>0.216574</td>\n",
              "      <td>0.475820</td>\n",
              "      <td>0.573975</td>\n",
              "      <td>0.787424</td>\n",
              "      <td>0.243529</td>\n",
              "      <td>0.093492</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_worst</th>\n",
              "      <td>0.064720</td>\n",
              "      <td>0.456903</td>\n",
              "      <td>0.297008</td>\n",
              "      <td>0.912045</td>\n",
              "      <td>0.303038</td>\n",
              "      <td>0.287489</td>\n",
              "      <td>0.036072</td>\n",
              "      <td>0.248133</td>\n",
              "      <td>0.299879</td>\n",
              "      <td>0.292752</td>\n",
              "      <td>0.090651</td>\n",
              "      <td>-0.051269</td>\n",
              "      <td>0.194799</td>\n",
              "      <td>0.409003</td>\n",
              "      <td>0.200371</td>\n",
              "      <td>0.196497</td>\n",
              "      <td>-0.074743</td>\n",
              "      <td>0.143003</td>\n",
              "      <td>0.100241</td>\n",
              "      <td>0.086741</td>\n",
              "      <td>-0.077473</td>\n",
              "      <td>-0.003195</td>\n",
              "      <td>0.359921</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.365098</td>\n",
              "      <td>0.345842</td>\n",
              "      <td>0.225429</td>\n",
              "      <td>0.360832</td>\n",
              "      <td>0.368366</td>\n",
              "      <td>0.359755</td>\n",
              "      <td>0.233027</td>\n",
              "      <td>0.219122</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_worst</th>\n",
              "      <td>0.079986</td>\n",
              "      <td>0.782914</td>\n",
              "      <td>0.965137</td>\n",
              "      <td>0.358040</td>\n",
              "      <td>0.970387</td>\n",
              "      <td>0.959120</td>\n",
              "      <td>0.238853</td>\n",
              "      <td>0.590210</td>\n",
              "      <td>0.729565</td>\n",
              "      <td>0.855923</td>\n",
              "      <td>0.219169</td>\n",
              "      <td>-0.205151</td>\n",
              "      <td>0.719684</td>\n",
              "      <td>-0.102242</td>\n",
              "      <td>0.721031</td>\n",
              "      <td>0.761213</td>\n",
              "      <td>-0.217304</td>\n",
              "      <td>0.260516</td>\n",
              "      <td>0.226680</td>\n",
              "      <td>0.394999</td>\n",
              "      <td>-0.103753</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.993708</td>\n",
              "      <td>0.365098</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.977578</td>\n",
              "      <td>0.236775</td>\n",
              "      <td>0.529408</td>\n",
              "      <td>0.618344</td>\n",
              "      <td>0.816322</td>\n",
              "      <td>0.269493</td>\n",
              "      <td>0.138957</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_worst</th>\n",
              "      <td>0.107187</td>\n",
              "      <td>0.733825</td>\n",
              "      <td>0.941082</td>\n",
              "      <td>0.343546</td>\n",
              "      <td>0.941550</td>\n",
              "      <td>0.959213</td>\n",
              "      <td>0.206718</td>\n",
              "      <td>0.509604</td>\n",
              "      <td>0.675987</td>\n",
              "      <td>0.809630</td>\n",
              "      <td>0.177193</td>\n",
              "      <td>-0.231854</td>\n",
              "      <td>0.751548</td>\n",
              "      <td>-0.083195</td>\n",
              "      <td>0.730713</td>\n",
              "      <td>0.811408</td>\n",
              "      <td>-0.182195</td>\n",
              "      <td>0.199371</td>\n",
              "      <td>0.188353</td>\n",
              "      <td>0.342271</td>\n",
              "      <td>-0.110343</td>\n",
              "      <td>-0.022736</td>\n",
              "      <td>0.984015</td>\n",
              "      <td>0.345842</td>\n",
              "      <td>0.977578</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.209145</td>\n",
              "      <td>0.438296</td>\n",
              "      <td>0.543331</td>\n",
              "      <td>0.747419</td>\n",
              "      <td>0.209146</td>\n",
              "      <td>0.079647</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_worst</th>\n",
              "      <td>0.010338</td>\n",
              "      <td>0.421465</td>\n",
              "      <td>0.119616</td>\n",
              "      <td>0.077503</td>\n",
              "      <td>0.150549</td>\n",
              "      <td>0.123523</td>\n",
              "      <td>0.805324</td>\n",
              "      <td>0.565541</td>\n",
              "      <td>0.448822</td>\n",
              "      <td>0.452753</td>\n",
              "      <td>0.426675</td>\n",
              "      <td>0.504942</td>\n",
              "      <td>0.141919</td>\n",
              "      <td>-0.073658</td>\n",
              "      <td>0.130054</td>\n",
              "      <td>0.125389</td>\n",
              "      <td>0.314457</td>\n",
              "      <td>0.227394</td>\n",
              "      <td>0.168481</td>\n",
              "      <td>0.215351</td>\n",
              "      <td>-0.012662</td>\n",
              "      <td>0.170568</td>\n",
              "      <td>0.216574</td>\n",
              "      <td>0.225429</td>\n",
              "      <td>0.236775</td>\n",
              "      <td>0.209145</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.568187</td>\n",
              "      <td>0.518523</td>\n",
              "      <td>0.547691</td>\n",
              "      <td>0.493838</td>\n",
              "      <td>0.617624</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_worst</th>\n",
              "      <td>-0.002968</td>\n",
              "      <td>0.590998</td>\n",
              "      <td>0.413463</td>\n",
              "      <td>0.277830</td>\n",
              "      <td>0.455774</td>\n",
              "      <td>0.390410</td>\n",
              "      <td>0.472468</td>\n",
              "      <td>0.865809</td>\n",
              "      <td>0.754968</td>\n",
              "      <td>0.667454</td>\n",
              "      <td>0.473200</td>\n",
              "      <td>0.458798</td>\n",
              "      <td>0.287103</td>\n",
              "      <td>-0.092439</td>\n",
              "      <td>0.341919</td>\n",
              "      <td>0.283257</td>\n",
              "      <td>-0.055558</td>\n",
              "      <td>0.678780</td>\n",
              "      <td>0.484858</td>\n",
              "      <td>0.452888</td>\n",
              "      <td>0.060255</td>\n",
              "      <td>0.390159</td>\n",
              "      <td>0.475820</td>\n",
              "      <td>0.360832</td>\n",
              "      <td>0.529408</td>\n",
              "      <td>0.438296</td>\n",
              "      <td>0.568187</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.892261</td>\n",
              "      <td>0.801080</td>\n",
              "      <td>0.614441</td>\n",
              "      <td>0.810455</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_worst</th>\n",
              "      <td>0.023203</td>\n",
              "      <td>0.659610</td>\n",
              "      <td>0.526911</td>\n",
              "      <td>0.301025</td>\n",
              "      <td>0.563879</td>\n",
              "      <td>0.512606</td>\n",
              "      <td>0.434926</td>\n",
              "      <td>0.816275</td>\n",
              "      <td>0.884103</td>\n",
              "      <td>0.752399</td>\n",
              "      <td>0.433721</td>\n",
              "      <td>0.346234</td>\n",
              "      <td>0.380585</td>\n",
              "      <td>-0.068956</td>\n",
              "      <td>0.418899</td>\n",
              "      <td>0.385100</td>\n",
              "      <td>-0.058298</td>\n",
              "      <td>0.639147</td>\n",
              "      <td>0.662564</td>\n",
              "      <td>0.549592</td>\n",
              "      <td>0.037119</td>\n",
              "      <td>0.379975</td>\n",
              "      <td>0.573975</td>\n",
              "      <td>0.368366</td>\n",
              "      <td>0.618344</td>\n",
              "      <td>0.543331</td>\n",
              "      <td>0.518523</td>\n",
              "      <td>0.892261</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.855434</td>\n",
              "      <td>0.532520</td>\n",
              "      <td>0.686511</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points_worst</th>\n",
              "      <td>0.035174</td>\n",
              "      <td>0.793566</td>\n",
              "      <td>0.744214</td>\n",
              "      <td>0.295316</td>\n",
              "      <td>0.771241</td>\n",
              "      <td>0.722017</td>\n",
              "      <td>0.503053</td>\n",
              "      <td>0.815573</td>\n",
              "      <td>0.861323</td>\n",
              "      <td>0.910155</td>\n",
              "      <td>0.430297</td>\n",
              "      <td>0.175325</td>\n",
              "      <td>0.531062</td>\n",
              "      <td>-0.119638</td>\n",
              "      <td>0.554897</td>\n",
              "      <td>0.538166</td>\n",
              "      <td>-0.102007</td>\n",
              "      <td>0.483208</td>\n",
              "      <td>0.440472</td>\n",
              "      <td>0.602450</td>\n",
              "      <td>-0.030413</td>\n",
              "      <td>0.215204</td>\n",
              "      <td>0.787424</td>\n",
              "      <td>0.359755</td>\n",
              "      <td>0.816322</td>\n",
              "      <td>0.747419</td>\n",
              "      <td>0.547691</td>\n",
              "      <td>0.801080</td>\n",
              "      <td>0.855434</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.502528</td>\n",
              "      <td>0.511114</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_worst</th>\n",
              "      <td>-0.044224</td>\n",
              "      <td>0.416294</td>\n",
              "      <td>0.163953</td>\n",
              "      <td>0.105008</td>\n",
              "      <td>0.189115</td>\n",
              "      <td>0.143570</td>\n",
              "      <td>0.394309</td>\n",
              "      <td>0.510223</td>\n",
              "      <td>0.409464</td>\n",
              "      <td>0.375744</td>\n",
              "      <td>0.699826</td>\n",
              "      <td>0.334019</td>\n",
              "      <td>0.094543</td>\n",
              "      <td>-0.128215</td>\n",
              "      <td>0.109930</td>\n",
              "      <td>0.074126</td>\n",
              "      <td>-0.107342</td>\n",
              "      <td>0.277878</td>\n",
              "      <td>0.197788</td>\n",
              "      <td>0.143116</td>\n",
              "      <td>0.389402</td>\n",
              "      <td>0.111094</td>\n",
              "      <td>0.243529</td>\n",
              "      <td>0.233027</td>\n",
              "      <td>0.269493</td>\n",
              "      <td>0.209146</td>\n",
              "      <td>0.493838</td>\n",
              "      <td>0.614441</td>\n",
              "      <td>0.532520</td>\n",
              "      <td>0.502528</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.537848</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <td>-0.029866</td>\n",
              "      <td>0.323872</td>\n",
              "      <td>0.007066</td>\n",
              "      <td>0.119205</td>\n",
              "      <td>0.051019</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>0.499316</td>\n",
              "      <td>0.687382</td>\n",
              "      <td>0.514930</td>\n",
              "      <td>0.368661</td>\n",
              "      <td>0.438413</td>\n",
              "      <td>0.767297</td>\n",
              "      <td>0.049559</td>\n",
              "      <td>-0.045655</td>\n",
              "      <td>0.085433</td>\n",
              "      <td>0.017539</td>\n",
              "      <td>0.101480</td>\n",
              "      <td>0.590973</td>\n",
              "      <td>0.439329</td>\n",
              "      <td>0.310655</td>\n",
              "      <td>0.078079</td>\n",
              "      <td>0.591328</td>\n",
              "      <td>0.093492</td>\n",
              "      <td>0.219122</td>\n",
              "      <td>0.138957</td>\n",
              "      <td>0.079647</td>\n",
              "      <td>0.617624</td>\n",
              "      <td>0.810455</td>\n",
              "      <td>0.686511</td>\n",
              "      <td>0.511114</td>\n",
              "      <td>0.537848</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unnamed: 32</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               id  ...  Unnamed: 32\n",
              "id                       1.000000  ...          NaN\n",
              "diagnosis                0.039769  ...          NaN\n",
              "radius_mean              0.074626  ...          NaN\n",
              "texture_mean             0.099770  ...          NaN\n",
              "perimeter_mean           0.073159  ...          NaN\n",
              "area_mean                0.096893  ...          NaN\n",
              "smoothness_mean         -0.012968  ...          NaN\n",
              "compactness_mean         0.000096  ...          NaN\n",
              "concavity_mean           0.050080  ...          NaN\n",
              "concave points_mean      0.044158  ...          NaN\n",
              "symmetry_mean           -0.022114  ...          NaN\n",
              "fractal_dimension_mean  -0.052511  ...          NaN\n",
              "radius_se                0.143048  ...          NaN\n",
              "texture_se              -0.007526  ...          NaN\n",
              "perimeter_se             0.137331  ...          NaN\n",
              "area_se                  0.177742  ...          NaN\n",
              "smoothness_se            0.096781  ...          NaN\n",
              "compactness_se           0.033961  ...          NaN\n",
              "concavity_se             0.055239  ...          NaN\n",
              "concave points_se        0.078768  ...          NaN\n",
              "symmetry_se             -0.017306  ...          NaN\n",
              "fractal_dimension_se     0.025725  ...          NaN\n",
              "radius_worst             0.082405  ...          NaN\n",
              "texture_worst            0.064720  ...          NaN\n",
              "perimeter_worst          0.079986  ...          NaN\n",
              "area_worst               0.107187  ...          NaN\n",
              "smoothness_worst         0.010338  ...          NaN\n",
              "compactness_worst       -0.002968  ...          NaN\n",
              "concavity_worst          0.023203  ...          NaN\n",
              "concave points_worst     0.035174  ...          NaN\n",
              "symmetry_worst          -0.044224  ...          NaN\n",
              "fractal_dimension_worst -0.029866  ...          NaN\n",
              "Unnamed: 32                   NaN  ...          NaN\n",
              "\n",
              "[33 rows x 33 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSR-dM3MLEfN"
      },
      "source": [
        "info.drop(['fractal_dimension_mean','texture_se','smoothness_se','symmetry_se','fractal_dimension_se'],axis=1,inplace=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-y7E-paLEiE"
      },
      "source": [
        "#last_label = np.array(last_label)\n",
        "#img_path = np.array(img_path)\n",
        "X=info.iloc[:,2:]\n",
        "y=info.diagnosis"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHGjV9uHLEp1"
      },
      "source": [
        "# split train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYaw83TOLEsu",
        "outputId": "c97760d4-655b-4d3e-d4f6-6ea40292f3c8"
      },
      "source": [
        "len(X_train),len(X_test),len(y_train),len(y_test)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(455, 114, 455, 114)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz4l78VVLM_f"
      },
      "source": [
        "#x_train = np.array(x_train)\n",
        "#x_test = np.array(x_test)\n",
        "X_tr = np.array(X_train).reshape(X_train.shape[0], X_train.shape[1], 1).astype('float32')\n",
        "X_vd = np.array(X_test).reshape(X_test.shape[0], X_test.shape[1], 1).astype('float32')\n",
        "\n",
        "y_tr = np.array(y_train)\n",
        "y_vd = np.array(y_test)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I346hBzLNCW",
        "outputId": "f76548de-678a-4768-bdae-72a65e7a83b9"
      },
      "source": [
        "X_tr.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(455, 26, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYbytb1KLNFa",
        "outputId": "c42ad305-ac52-434d-9192-a107b5112ecf"
      },
      "source": [
        "print(\"\\n\")\n",
        "print(\"-----------------RNN-----------------\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "RNN = Sequential()\n",
        "\n",
        "RNN.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "RNN.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "RNN.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "RNN.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "history=RNN.fit(X_train, y_train, batch_size = 10, epochs = 20,verbose = 2)\n",
        "\n",
        "y_pred = RNN.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "\"Analysis Report\"\n",
        "print()\n",
        "print(\"------Classification Report------\")\n",
        "print(classification_report(y_pred,y_test))\n",
        "\n",
        "print()\n",
        "print(\"------Classification Report------\")\n",
        "print(confusion_matrix(y_pred,y_test))\n",
        "\n",
        "print()\n",
        "print(\"------Accuracy------\")\n",
        "print(f\" Accuracy_RNN :{(accuracy_score(y_pred,y_test)*100)}\")\n",
        "print()\n",
        "\n",
        "val_loss, val_acc = RNN.evaluate(X_test,y_test)\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "-----------------RNN-----------------\n",
            "\n",
            "\n",
            "Epoch 1/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 2/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 3/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 4/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 5/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 6/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 7/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 8/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 9/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 10/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 11/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 12/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 13/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 14/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 15/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 16/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 17/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 18/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 19/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "Epoch 20/20\n",
            "46/46 - 0s - loss: nan - accuracy: 0.6264\n",
            "\n",
            "------Classification Report------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.63      0.77       114\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.63       114\n",
            "   macro avg       0.50      0.32      0.39       114\n",
            "weighted avg       1.00      0.63      0.77       114\n",
            "\n",
            "\n",
            "------Classification Report------\n",
            "[[72 42]\n",
            " [ 0  0]]\n",
            "\n",
            "------Accuracy------\n",
            " Accuracy_RNN :63.1578947368421\n",
            "\n",
            "4/4 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm2m2PjELNHd",
        "outputId": "b58b15e8-e327-4699-95c6-61f1c2dd2f80"
      },
      "source": [
        "acc_RNN=val_acc*100\n",
        "print(acc_RNN)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63.1578947368421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtQsjCvKLExg"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e-FqXBBlMfi"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhCVYa3v6d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "ebc8b600-4933-4941-f905-7cd05a67346b"
      },
      "source": [
        "results = pd.DataFrame({\n",
        "    'Model': ['CNN','Fuzzy','RNN'],\n",
        "    'Score': [acc_CNN,acc_fuzzy,acc_RNN]})\n",
        "result_df = results.sort_values(by='Score', ascending=False)\n",
        "#result_df = result_df.set_index('Score')\n",
        "result_df.head(9)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CNN</td>\n",
              "      <td>95.610000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fuzzy</td>\n",
              "      <td>88.810000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RNN</td>\n",
              "      <td>63.157895</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Model      Score\n",
              "0    CNN  95.610000\n",
              "1  Fuzzy  88.810000\n",
              "2    RNN  63.157895"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNy0NeXaFgMF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "18334e0d-e5d5-4cbf-aeb4-d55c6c6e7e65"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(result_df['Model'],result_df['Score']);"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn5TEfuhPAC-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "outputId": "ca528686-7868-4672-b007-0fefdccbe4e9"
      },
      "source": [
        "# Passing the parameters to the bar function, this is the main function which creates the bar plot\n",
        "plt.figure(figsize=(12,7))\n",
        "Model=result_df['Model'].tolist()\n",
        "Score=result_df['Score'].tolist()\n",
        "plt.bar(Model, Score, width= 0.9, align='center',color='cyan', edgecolor = 'red')\n",
        "# This is the location for the annotated text\n",
        "i = 4.0\n",
        "j = 1\n",
        "# Annotating the bar plot with the values (total death count)\n",
        "for i in range(len(Model)):\n",
        "    plt.annotate(Score[i], (-0.1 + i, Score[i] + j))\n",
        "# Creating the legend of the bars in the plot\n",
        "plt.legend(labels = ['Accuracy'])\n",
        "# Giving the tilte for the plot\n",
        "plt.title(\"Bar plot representing Breast Cancer Models Prediction\")\n",
        "# Namimg the x and y axis\n",
        "plt.xlabel('Algorithm')\n",
        "plt.ylabel('Accuracy')\n",
        "# Saving the plot as a 'png'\n",
        "#plt.savefig('1BarPlot.png')\n",
        "# Displaying the bar plot\n",
        "#plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Accuracy')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}